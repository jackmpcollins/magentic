{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>Easily integrate Large Language Models into your Python code. Simply use the <code>@prompt</code> decorator to create functions that return structured output from the LLM. Mix LLM queries and function calling with regular Python code to create complex logic.</p> <p><code>magentic</code> is</p> <ul> <li>Compact: Query LLMs without duplicating boilerplate code.</li> <li>Atomic: Prompts are functions that can be individually tested and reasoned about.</li> <li>Transparent: Create \"chains\" using regular Python code. Define all of your own prompts.</li> <li>Compatible: Use <code>@prompt</code> functions as normal functions, including with decorators like <code>@lru_cache</code>.</li> <li>Type Annotated: Works with linters and IDEs.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install magentic\n</code></pre> <p>or using poetry</p> <pre><code>poetry add magentic\n</code></pre> <p>Configure your OpenAI API key by setting the <code>OPENAI_API_KEY</code> environment variable or using <code>openai.api_key = \"sk-...\"</code>. See the OpenAI Python library documentation for more information.</p>"},{"location":"#usage","title":"Usage","text":""},{"location":"#prompt","title":"@prompt","text":"<p>The <code>@prompt</code> decorator allows you to define a template for a Large Language Model (LLM) prompt as a Python function. When this function is called, the arguments are inserted into the template, then this prompt is sent to an LLM which generates the function output.</p> <pre><code>from magentic import prompt\n\n\n@prompt('Add more \"dude\"ness to: {phrase}')\ndef dudeify(phrase: str) -&gt; str:\n    ...  # No function body as this is never executed\n\n\ndudeify(\"Hello, how are you?\")\n# \"Hey, dude! What's up? How's it going, my man?\"\n</code></pre> <p>The <code>@prompt</code> decorator will respect the return type annotation of the decorated function. This can be any type supported by pydantic including a <code>pydantic</code> model.</p> <pre><code>from magentic import prompt\nfrom pydantic import BaseModel\n\n\nclass Superhero(BaseModel):\n    name: str\n    age: int\n    power: str\n    enemies: list[str]\n\n\n@prompt(\"Create a Superhero named {name}.\")\ndef create_superhero(name: str) -&gt; Superhero:\n    ...\n\n\ncreate_superhero(\"Garden Man\")\n# Superhero(name='Garden Man', age=30, power='Control over plants', enemies=['Pollution Man', 'Concrete Woman'])\n</code></pre>"},{"location":"#functioncall","title":"FunctionCall","text":"<p>An LLM can also decide to call functions. In this case the <code>@prompt</code>-decorated function returns a <code>FunctionCall</code> object which can be called to execute the function using the arguments provided by the LLM.</p> <pre><code>from typing import Literal\n\nfrom magentic import prompt, FunctionCall\n\n\ndef activate_oven(temperature: int, mode: Literal[\"broil\", \"bake\", \"roast\"]) -&gt; str:\n    \"\"\"Turn the oven on with the provided settings.\"\"\"\n    return f\"Preheating to {temperature} F with mode {mode}\"\n\n\n@prompt(\n    \"Prepare the oven so I can make {food}\",\n    functions=[activate_oven],\n)\ndef configure_oven(food: str) -&gt; FunctionCall[str]:\n    ...\n\n\noutput = configure_oven(\"cookies!\")\n# FunctionCall(&lt;function activate_oven at 0x1105a6200&gt;, temperature=350, mode='bake')\noutput()\n# 'Preheating to 350 F with mode bake'\n</code></pre>"},{"location":"#prompt_chain","title":"@prompt_chain","text":"<p>Sometimes the LLM requires making one or more function calls to generate a final answer. The <code>@prompt_chain</code> decorator will resolve <code>FunctionCall</code> objects automatically and pass the output back to the LLM to continue until the final answer is reached.</p> <p>In the following example, when <code>describe_weather</code> is called the LLM first calls the <code>get_current_weather</code> function, then uses the result of this to formulate its final answer which gets returned.</p> <pre><code>from magentic import prompt_chain\n\n\ndef get_current_weather(location, unit=\"fahrenheit\"):\n    \"\"\"Get the current weather in a given location\"\"\"\n    # Pretend to query an API\n    return {\n        \"location\": location,\n        \"temperature\": \"72\",\n        \"unit\": unit,\n        \"forecast\": [\"sunny\", \"windy\"],\n    }\n\n\n@prompt_chain(\n    \"What's the weather like in {city}?\",\n    functions=[get_current_weather],\n)\ndef describe_weather(city: str) -&gt; str:\n    ...\n\n\ndescribe_weather(\"Boston\")\n# 'The current weather in Boston is 72\u00b0F and it is sunny and windy.'\n</code></pre> <p>LLM-powered functions created using <code>@prompt</code> and <code>@prompt_chain</code> can be supplied as <code>functions</code> to other <code>@prompt</code>/<code>@prompt_chain</code> decorators, just like regular python functions. This enables increasingly complex LLM-powered functionality, while allowing individual components to be tested and improved in isolation.</p>"},{"location":"asyncio/","title":"Asyncio","text":"<p>Asynchronous functions / coroutines can be used to concurrently query the LLM. This can greatly increase the overall speed of generation, and also allow other asynchronous code to run while waiting on LLM output. In the below example, the LLM generates a description for each US president while it is waiting on the next one in the list. Measuring the characters generated per second shows that this example achieves a 7x speedup over serial processing.</p> <pre><code>import asyncio\nfrom time import time\nfrom typing import AsyncIterable\n\nfrom magentic import prompt\n\n\n@prompt(\"List ten presidents of the United States\")\nasync def iter_presidents() -&gt; AsyncIterable[str]:\n    ...\n\n\n@prompt(\"Tell me more about {topic}\")\nasync def tell_me_more_about(topic: str) -&gt; str:\n    ...\n\n\n# For each president listed, generate a description concurrently\nstart_time = time()\ntasks = []\nasync for president in await iter_presidents():\n    # Use asyncio.create_task to schedule the coroutine for execution before awaiting it\n    # This way descriptions will start being generated while the list of presidents is still being generated\n    task = asyncio.create_task(tell_me_more_about(president))\n    tasks.append(task)\n\ndescriptions = await asyncio.gather(*tasks)\n\n# Measure the characters per second\ntotal_chars = sum(len(desc) for desc in descriptions)\ntime_elapsed = time() - start_time\nprint(total_chars, time_elapsed, total_chars / time_elapsed)\n# 24575 28.70 856.07\n\n\n# Measure the characters per second to describe a single president\nstart_time = time()\nout = await tell_me_more_about(\"George Washington\")\ntime_elapsed = time() - start_time\nprint(len(out), time_elapsed, len(out) / time_elapsed)\n# 2206 18.72 117.78\n</code></pre>"},{"location":"chat-prompting/","title":"Chat Prompting","text":""},{"location":"chat-prompting/#chatprompt","title":"@chatprompt","text":"<p>The <code>@chatprompt</code> decorator works just like <code>@prompt</code> but allows you to pass chat messages as a template rather than a single text prompt. This can be used to provide a system message or for few-shot prompting where you provide example responses to guide the model's output. Format fields denoted by curly braces <code>{example}</code> will be filled in all messages (except <code>FunctionResultMessage</code>).</p> <pre><code>from magentic import chatprompt, AssistantMessage, SystemMessage, UserMessage\nfrom pydantic import BaseModel\n\n\nclass Quote(BaseModel):\n    quote: str\n    character: str\n\n\n@chatprompt(\n    SystemMessage(\"You are a movie buff.\"),\n    UserMessage(\"What is your favorite quote from Harry Potter?\"),\n    AssistantMessage(\n        Quote(\n            quote=\"It does not do to dwell on dreams and forget to live.\",\n            character=\"Albus Dumbledore\",\n        )\n    ),\n    UserMessage(\"What is your favorite quote from {movie}?\"),\n)\ndef get_movie_quote(movie: str) -&gt; Quote:\n    ...\n\n\nget_movie_quote(\"Iron Man\")\n# Quote(quote='I am Iron Man.', character='Tony Stark')\n</code></pre>"},{"location":"chat-prompting/#escape_braces","title":"escape_braces","text":"<p>To prevent curly braces from being interpreted as format fields, use the <code>escape_braces</code> function to escape them in strings.</p> <pre><code>from magentic.chatprompt import escape_braces\n\nstring_with_braces = \"Curly braces like {example} will be filled in!\"\nescaped_string = escape_braces(string_with_braces)\n# 'Curly braces {{example}} will be filled in!'\nescaped_string.format(example=\"test\")\n# 'Curly braces {example} will be filled in!'\n</code></pre>"},{"location":"chat-prompting/#placeholder","title":"Placeholder","text":"<p>The <code>Placeholder</code> class enables templating of <code>AssistantMessage</code> content within the <code>@chatprompt</code> decorator. This allows dynamic changing of the messages used to prompt the model based on the arguments provided when the function is called.</p> <pre><code>from magentic import chatprompt, AssistantMessage, Placeholder, UserMessage\nfrom pydantic import BaseModel\n\n\nclass Quote(BaseModel):\n    quote: str\n    character: str\n\n\n@chatprompt(\n    UserMessage(\"Tell me a quote from {movie}\"),\n    AssistantMessage(Placeholder(Quote, \"quote\")),\n    UserMessage(\"What is a similar quote from the same movie?\"),\n)\ndef get_similar_quote(movie: str, quote: Quote) -&gt; Quote:\n    ...\n\n\nget_similar_quote(\n    movie=\"Star Wars\",\n    quote=Quote(quote=\"I am your father\", character=\"Darth Vader\"),\n)\n# Quote(quote='The Force will be with you, always.', character='Obi-Wan Kenobi')\n</code></pre> <p><code>Placeholder</code> can also be utilized in the <code>format</code> method of custom <code>Message</code> subclasses to provide an explicit way of inserting values from the function arguments. For example, see <code>UserImageMessage</code> in (TODO: link to GPT-vision page).</p>"},{"location":"chat-prompting/#functioncall","title":"FunctionCall","text":"<p>The content of an <code>AssistantMessage</code> can be a <code>FunctionCall</code>. This can be used to demonstrate to the LLM when/how it should call a function.</p> <pre><code>from magentic import (\n    chatprompt,\n    AssistantMessage,\n    FunctionCall,\n    UserMessage,\n    SystemMessage,\n)\n\n\ndef change_music_volume(increment: int):\n    \"\"\"Change music volume level. Min 1, max 10.\"\"\"\n    print(f\"Music volume change: {increment}\")\n\n\ndef order_food(food: str, amount: int):\n    \"\"\"Order food.\"\"\"\n    print(f\"Ordered {amount} {food}\")\n\n\n@chatprompt(\n    SystemMessage(\n        \"You are hosting a party and must keep the guests happy.\"\n        \"Call functions as needed. Do not respond directly.\"\n    ),\n    UserMessage(\"It's pretty loud in here!\"),\n    AssistantMessage(FunctionCall(change_music_volume, -2)),\n    UserMessage(\"{request}\"),\n    functions=[change_music_volume, order_food],\n)\ndef adjust_for_guest(request: str) -&gt; FunctionCall[None]:\n    ...\n\n\nfunc = adjust_for_guest(\"Do you have any more food?\")\nfunc()\n# Ordered 3 pizza\n</code></pre> <p>To include the result of calling the function in the messages use a <code>FunctionResultMessage</code>.</p>"},{"location":"configuration/","title":"LLM Configuration","text":"<p>Currently two backends are available</p> <ul> <li><code>openai</code> : the default backend that uses the <code>openai</code> Python package. Supports all features.</li> <li><code>litellm</code> : uses the <code>litellm</code> Python package to enable querying LLMs from many different providers. Install this with <code>pip install magentic[litellm]</code>. Note: some models may not support all features of <code>magentic</code> e.g. function calling/structured output and streaming.</li> </ul> <p>The backend and LLM used by <code>magentic</code> can be configured in several ways. The order of precedence of configuration is</p> <ol> <li>Arguments explicitly passed when initializing an instance in Python</li> <li>Values set using a context manager in Python</li> <li>Environment variables</li> <li>Default values from src/magentic/settings.py</li> </ol> <pre><code>from magentic import OpenaiChatModel, prompt\nfrom magentic.chat_model.litellm_chat_model import LitellmChatModel\n\n\n@prompt(\"Say hello\")\ndef say_hello() -&gt; str:\n    ...\n\n\n@prompt(\n    \"Say hello\",\n    model=LitellmChatModel(\"ollama/llama2\"),\n)\ndef say_hello_litellm() -&gt; str:\n    ...\n\n\nsay_hello()  # Uses env vars or default settings\n\nwith OpenaiChatModel(\"gpt-3.5-turbo\", temperature=1):\n    say_hello()  # Uses openai with gpt-3.5-turbo and temperature=1 due to context manager\n    say_hello_litellm()  # Uses litellm with ollama/llama2 because explicitly configured\n</code></pre> <p>The following environment variables can be set.</p> Environment Variable Description Example MAGENTIC_BACKEND The package to use as the LLM backend openai MAGENTIC_LITELLM_MODEL LiteLLM model claude-2 MAGENTIC_LITELLM_API_BASE The base url to query http://localhost:11434 MAGENTIC_LITELLM_MAX_TOKENS LiteLLM max number of generated tokens 1024 MAGENTIC_LITELLM_TEMPERATURE LiteLLM temperature 0.5 MAGENTIC_OPENAI_MODEL OpenAI model gpt-4 MAGENTIC_OPENAI_API_KEY OpenAI API key to be used by magentic sk-... MAGENTIC_OPENAI_API_TYPE Allowed options: \"openai\", \"azure\" azure MAGENTIC_OPENAI_BASE_URL Base URL for an OpenAI-compatible API http://localhost:8080 MAGENTIC_OPENAI_MAX_TOKENS OpenAI max number of generated tokens 1024 MAGENTIC_OPENAI_SEED Seed for deterministic sampling 42 MAGENTIC_OPENAI_TEMPERATURE OpenAI temperature 0.5 <p>When using the <code>openai</code> backend, setting the <code>MAGENTIC_OPENAI_BASE_URL</code> environment variable or using <code>OpenaiChatModel(..., base_url=\"http://localhost:8080\")</code> in code allows you to use <code>magentic</code> with any OpenAI-compatible API e.g. Azure OpenAI Service, LiteLLM OpenAI Proxy Server, LocalAI. Note that if the API does not support function calling then you will not be able to create prompt-functions that return Python objects, but other features of <code>magentic</code> will still work.</p> <p>To use Azure with the openai backend you will need to set the <code>MAGENTIC_OPENAI_API_TYPE</code> environment variable to \"azure\" or use <code>OpenaiChatModel(..., api_type=\"azure\")</code>, and also set the environment variables needed by the openai package to access Azure. See https://github.com/openai/openai-python#microsoft-azure-openai</p>"},{"location":"formatting/","title":"Formatting","text":""},{"location":"formatting/#the-format-method","title":"The <code>format</code> Method","text":"<p>Functions created using magentic decorators expose a <code>format</code> method that accepts the same parameters as the function itself but returns the completed prompt that will be sent to the model. For <code>@prompt</code> this method returns a string, and for <code>@chatprompt</code> it returns a list of <code>Message</code> objects. The <code>format</code> method can be used to test that the final prompt created by a magentic function is formatted as expected.</p> <pre><code>from magentic import prompt\n\n\n@prompt(\"Write a short poem about {topic}.\")\ndef create_poem(topic: str) -&gt; str:\n    ...\n\n\ncreate_poem.format(\"fruit\")\n# 'Write a short poem about fruit.'\n</code></pre>"},{"location":"formatting/#classes-for-formatting","title":"Classes for Formatting","text":"<p>By default, when a list is used in a prompt template string it is formatted using its Python representation.</p> <pre><code>from magentic import prompt\nfrom magentic.formatting import BulletedList\n\n\n@prompt(\"Continue the list:\\n{items}\")\ndef get_next_items(items: list[str]) -&gt; list[str]:\n    ...\n\n\nitems = [\"apple\", \"banana\", \"cherry\"]\nprint(get_next_items.format(items=items))\n# Continue the list:\n# ['apple', 'banana', 'cherry']\n</code></pre> <p>However, the LLM might respond better to a prompt in which the list is formatted more clearly or the items are numbered. The <code>BulletedList</code>, <code>NumberedList</code>, <code>BulletedDict</code> and <code>NumberedDict</code> classes are provided to enable this.</p> <p>For example, to modify the above prompt to contain a numbered list of the items, the <code>NumberedList</code> class can be used. This behaves exactly like a regular Python <code>list</code> except for how it appears when inserted into a formatted string. This class can also be used as the type annotation for <code>items</code> parameter to ensure that this prompt always contains a numbered list.</p> <pre><code>from magentic import prompt\nfrom magentic.formatting import NumberedList\n\n\n@prompt(\"Continue the list:\\n{items}\")\ndef get_next_items(items: NumberedList[str]) -&gt; list[str]:\n    ...\n\n\nitems = NumberedList([\"apple\", \"banana\", \"cherry\"])\nprint(get_next_items.format(items=items))\n# Continue the list:\n# 1. apple\n# 2. banana\n# 3. cherry\n</code></pre>"},{"location":"formatting/#custom-formatting","title":"Custom Formatting","text":"<p>When objects are inserted into formatted strings in Python, the <code>__format__</code> method is called. By defining or modifying this method you can control how an object is converted to a string in the prompt. If you own the class you can modify the <code>__format__</code> method directly. Otherwise for third-party classes you will need to create a subcless.</p> <p>Here's an example of how to represent a dictionary as a bulleted list.</p> <pre><code>from typing import TypeVar\n\nfrom magentic import prompt\n\nK = TypeVar(\"K\")\nV = TypeVar(\"V\")\n\n\nclass BulletedDict(dict[K, V]):\n    def __format__(self, format_spec: str) -&gt; str:\n        # Here, you could use 'format_spec' to customize the formatting further if needed\n        return \"\\n\".join(f\"- {key}: {value}\" for key, value in self.items())\n\n\n@prompt(\"Identify the odd one out:\\n{items}\")\ndef find_odd_one_out(items: BulletedDict[str, str]) -&gt; str:\n    ...\n\n\nitems = BulletedDict({\"sky\": \"blue\", \"grass\": \"green\", \"sun\": \"purple\"})\nprint(find_odd_one_out.format(items))\n# Identify the odd one out:\n# - sky: blue\n# - grass: green\n# - sun: purple\n</code></pre>"},{"location":"streaming/","title":"Streaming","text":"<p>The <code>StreamedStr</code> (and <code>AsyncStreamedStr</code>) class can be used to stream the output of the LLM. This allows you to process the text while it is being generated, rather than receiving the whole output at once.</p> <pre><code>from magentic import prompt, StreamedStr\n\n\n@prompt(\"Tell me about {country}\")\ndef describe_country(country: str) -&gt; StreamedStr:\n    ...\n\n\n# Print the chunks while they are being received\nfor chunk in describe_country(\"Brazil\"):\n    print(chunk, end=\"\")\n# 'Brazil, officially known as the Federative Republic of Brazil, is ...'\n</code></pre> <p>Multiple <code>StreamedStr</code> can be created at the same time to stream LLM outputs concurrently. In the below example, generating the description for multiple countries takes approximately the same amount of time as for a single country.</p> <pre><code>from time import time\n\ncountries = [\"Australia\", \"Brazil\", \"Chile\"]\n\n\n# Generate the descriptions one at a time\nstart_time = time()\nfor country in countries:\n    # Converting `StreamedStr` to `str` blocks until the LLM output is fully generated\n    description = str(describe_country(country))\n    print(f\"{time() - start_time:.2f}s : {country} - {len(description)} chars\")\n\n# 22.72s : Australia - 2130 chars\n# 41.63s : Brazil - 1884 chars\n# 74.31s : Chile - 2968 chars\n\n\n# Generate the descriptions concurrently by creating the StreamedStrs at the same time\nstart_time = time()\nstreamed_strs = [describe_country(country) for country in countries]\nfor country, streamed_str in zip(countries, streamed_strs):\n    description = str(streamed_str)\n    print(f\"{time() - start_time:.2f}s : {country} - {len(description)} chars\")\n\n# 22.79s : Australia - 2147 chars\n# 23.64s : Brazil - 2202 chars\n# 24.67s : Chile - 2186 chars\n</code></pre>"},{"location":"streaming/#object-streaming","title":"Object Streaming","text":"<p>Structured outputs can also be streamed from the LLM by using the return type annotation <code>Iterable</code> (or <code>AsyncIterable</code>). This allows each item to be processed while the next one is being generated. See the example in examples/quiz for how this can be used to improve user experience by quickly displaying/using the first item returned.</p> <pre><code>from collections.abc import Iterable\nfrom time import time\n\nfrom magentic import prompt\nfrom pydantic import BaseModel\n\n\nclass Superhero(BaseModel):\n    name: str\n    age: int\n    power: str\n    enemies: list[str]\n\n\n@prompt(\"Create a Superhero team named {name}.\")\ndef create_superhero_team(name: str) -&gt; Iterable[Superhero]:\n    ...\n\n\nstart_time = time()\nfor hero in create_superhero_team(\"The Food Dudes\"):\n    print(f\"{time() - start_time:.2f}s : {hero}\")\n\n# 2.23s : name='Pizza Man' age=30 power='Can shoot pizza slices from his hands' enemies=['The Hungry Horde', 'The Junk Food Gang']\n# 4.03s : name='Captain Carrot' age=35 power='Super strength and agility from eating carrots' enemies=['The Sugar Squad', 'The Greasy Gang']\n# 6.05s : name='Ice Cream Girl' age=25 power='Can create ice cream out of thin air' enemies=['The Hot Sauce Squad', 'The Healthy Eaters']\n</code></pre>"},{"location":"type-checking/","title":"Type Checking","text":"<p>Many type checkers will raise warnings or errors for functions with the <code>@prompt</code> decorator due to the function having no body or return value. There are several ways to deal with these.</p> <ol> <li>Disable the check globally for the type checker. For example in mypy by disabling error code <code>empty-body</code>.    <pre><code># pyproject.toml\n[tool.mypy]\ndisable_error_code = [\"empty-body\"]\n</code></pre></li> <li>Make the function body <code>...</code> (this does not satisfy mypy) or <code>raise</code>.    <pre><code>@prompt(\"Choose a color\")\ndef random_color() -&gt; str:\n    ...\n</code></pre></li> <li>Use comment <code># type: ignore[empty-body]</code> on each function. In this case you can add a docstring instead of <code>...</code>.    <pre><code>@prompt(\"Choose a color\")\ndef random_color() -&gt; str:  # type: ignore[empty-body]\n    \"\"\"Returns a random color.\"\"\"\n</code></pre></li> </ol>"},{"location":"vision/","title":"Vision","text":"<p>GPT-4 Vision can be used with magentic by using the <code>UserImageMessage</code> message type. This allows the LLM to accept images as input. Currently this is only supported with the OpenAI backend (<code>OpenaiChatModel</code> with <code>\"gpt-4-vision-preview\"</code>).</p> <p>Return types</p> <p>GPT-4 Vision currently does not support function-calling/tools so functions using <code>@chatprompt</code> can only return <code>str</code>, <code>StreamedStr</code>, or <code>AsyncStreamedStr</code>.</p> <p><code>max_tokens</code></p> <p>By default <code>max_tokens</code> has a low value, so you will likely need to increase it.</p> <p>For more information visit the OpenAI Vision API documentation.</p>"},{"location":"vision/#userimagemessage","title":"UserImageMessage","text":"<p>The <code>UserImageMessage</code> can be used with <code>@chatprompt</code> alongside other messages. The LLM must be set to OpenAI's GPT4 Vision model <code>OpenaiChatModel(\"gpt-4-vision-preview\")</code>. This can be done by passing the <code>model</code> parameter to <code>@chatprompt</code>, or through the other methods of configuration.</p> <pre><code>from magentic import chatprompt, OpenaiChatModel, UserMessage\nfrom magentic.vision import UserImageMessage\n\n\nIMAGE_URL_WOODEN_BOARDWALK = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n\n\n@chatprompt(\n    UserMessage(\"Describe the following image in one sentence.\"),\n    UserImageMessage(IMAGE_URL_WOODEN_BOARDWALK),\n    model=OpenaiChatModel(\"gpt-4-vision-preview\", max_tokens=2000),\n)\ndef describe_image() -&gt; str:\n    ...\n\n\ndescribe_image()\n# 'A wooden boardwalk meanders through a lush green meadow under a blue sky with wispy clouds.'\n</code></pre>"},{"location":"vision/#placeholder","title":"Placeholder","text":"<p>To provide the image as a function parameter, use <code>Placeholder</code>. This substitutes a function argument into the message when the function is called.</p> <pre><code>from magentic import chatprompt, OpenaiChatModel, Placeholder, UserMessage\nfrom magentic.vision import UserImageMessage\n\n\nIMAGE_URL_WOODEN_BOARDWALK = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n\n\n@chatprompt(\n    UserMessage(\"Describe the following image in one sentence.\"),\n    UserImageMessage(Placeholder(str, \"image_url\")),\n    model=OpenaiChatModel(\"gpt-4-vision-preview\", max_tokens=2000),\n)\ndef describe_image(image_url: str) -&gt; str:\n    ...\n\n\ndescribe_image(IMAGE_URL_WOODEN_BOARDWALK)\n# 'A wooden boardwalk meanders through lush green wetlands under a partly cloudy blue sky.'\n</code></pre>"},{"location":"vision/#bytes","title":"bytes","text":"<p><code>UserImageMessage</code> can also accept <code>bytes</code> as input. Like <code>str</code>, this can be passed directly or via <code>Placeholder</code>.</p> <pre><code>import requests\n\nfrom magentic import chatprompt, OpenaiChatModel, Placeholder, UserMessage\nfrom magentic.vision import UserImageMessage\n\n\nIMAGE_URL_WOODEN_BOARDWALK = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n\n\ndef url_to_bytes(url: str) -&gt; bytes:\n    \"\"\"Get the content of a URL as bytes.\"\"\"\n    return requests.get(url).content\n\n\n@chatprompt(\n    UserMessage(\"Describe the following image in one sentence.\"),\n    UserImageMessage(Placeholder(bytes, \"image_bytes\")),\n    model=OpenaiChatModel(\"gpt-4-vision-preview\", max_tokens=2000),\n)\ndef describe_image(image_bytes: bytes) -&gt; str:\n    ...\n\n\nimage_bytes = url_to_bytes(IMAGE_URL_WOODEN_BOARDWALK)\ndescribe_image(image_bytes)\n# 'The image shows a wooden boardwalk extending through a lush green wetland with a backdrop of blue skies and scattered clouds.'\n</code></pre>"},{"location":"examples/chain_of_verification/","title":"Chain of Verification (CoVe)","text":"In\u00a0[1]: Copied! <pre># Define the prompts\n\nimport asyncio\n\nfrom magentic import prompt\n\n\n@prompt(\"{query}\")\nasync def answer_query(query: str) -&gt; str:\n    ...\n\n\n@prompt(\n    \"\"\"\\\nQuery: {query}\nResponse: {response}\n\nProvide specific questions to verify the facts in the above response as related to the query.\n\"\"\"\n)\nasync def generate_verification_questions(query: str, response: str) -&gt; list[str]:\n    ...\n\n\n@prompt(\n    \"\"\"\\\n{context}\n\nGiven the above context, what is the answer to the following question?\n\n{query}\"\"\"\n)\nasync def answer_query_with_context(query: str, context: str) -&gt; str:\n    ...\n</pre> # Define the prompts  import asyncio  from magentic import prompt   @prompt(\"{query}\") async def answer_query(query: str) -&gt; str:     ...   @prompt(     \"\"\"\\ Query: {query} Response: {response}  Provide specific questions to verify the facts in the above response as related to the query. \"\"\" ) async def generate_verification_questions(query: str, response: str) -&gt; list[str]:     ...   @prompt(     \"\"\"\\ {context}  Given the above context, what is the answer to the following question?  {query}\"\"\" ) async def answer_query_with_context(query: str, context: str) -&gt; str:     ... In\u00a0[2]: Copied! <pre># 1. Generate Baseline Response\n# Given a query, generate the response using the LLM.\n\nquery = \"Name some politicians who were born in NY, New York\"\n\nbaseline_response = await answer_query(query)\nprint(baseline_response)\n</pre> # 1. Generate Baseline Response # Given a query, generate the response using the LLM.  query = \"Name some politicians who were born in NY, New York\"  baseline_response = await answer_query(query) print(baseline_response) <pre>Here are some politicians who were born in New York, New York:\n\n1. Franklin D. Roosevelt - 32nd President of the United States.\n2. Theodore Roosevelt - 26th President of the United States.\n3. Donald Trump - 45th President of the United States.\n4. Hillary Clinton - Former Secretary of State and Democratic nominee for President in 2016.\n5. Michael Bloomberg - Former Mayor of New York City and businessman.\n6. Rudy Giuliani - Former Mayor of New York City and attorney.\n7. Chuck Schumer - U.S. Senator from New York and current Senate Majority Leader.\n8. Kirsten Gillibrand - U.S. Senator from New York.\n9. Mario Cuomo - Former Governor of New York.\n10. Andrew Cuomo - Current Governor of New York.\n\nPlease note that this is not an exhaustive list, and there are many more politicians who were born in New York, New York.\n</pre> In\u00a0[3]: Copied! <pre># 2. Plan Verifications\n# Given both query and baseline response, generate a list of verification questions\n# that could help to self-analyze if there are any mistakes in the original response.\n\nverification_questions = await generate_verification_questions(query, baseline_response)\n\nfor q in verification_questions:\n    print(q)\n</pre> # 2. Plan Verifications # Given both query and baseline response, generate a list of verification questions # that could help to self-analyze if there are any mistakes in the original response.  verification_questions = await generate_verification_questions(query, baseline_response)  for q in verification_questions:     print(q) <pre>Was Franklin D. Roosevelt born in New York, New York?\nWas Theodore Roosevelt born in New York, New York?\nWas Donald Trump born in New York, New York?\nWas Hillary Clinton born in New York, New York?\nWas Michael Bloomberg born in New York, New York?\nWas Rudy Giuliani born in New York, New York?\nWas Chuck Schumer born in New York, New York?\nWas Kirsten Gillibrand born in New York, New York?\nWas Mario Cuomo born in New York, New York?\nIs Andrew Cuomo the current Governor of New York?\n</pre> In\u00a0[4]: Copied! <pre># 3. Execute Verifications\n# Answer each verification question in turn, and hence check the answer against the\n# original response to check for inconsistencies or mistakes.\n\nverification_answers = await asyncio.gather(\n    *(answer_query(question) for question in verification_questions)\n)\n\nfor ans in verification_answers:\n    print(ans)\n</pre> # 3. Execute Verifications # Answer each verification question in turn, and hence check the answer against the # original response to check for inconsistencies or mistakes.  verification_answers = await asyncio.gather(     *(answer_query(question) for question in verification_questions) )  for ans in verification_answers:     print(ans) <pre>Yes, Franklin D. Roosevelt was born on January 30, 1882, in Hyde Park, New York, which is located in Dutchess County.\nYes, Theodore Roosevelt was born in New York City, New York on October 27, 1858. Specifically, he was born in a house located at 28 East 20th Street in Manhattan.\nYes, Donald Trump was indeed born in New York, New York on June 14, 1946.\nNo, Hillary Clinton was not born in New York, New York. She was born on October 26, 1947, in Chicago, Illinois.\nNo, Michael Bloomberg was born in Boston, Massachusetts on February 14, 1942.\nYes, Rudy Giuliani was born in New York, New York. He was born on May 28, 1944, in the Brooklyn borough of New York City.\nYes, Chuck Schumer was born in Brooklyn, New York on November 23, 1950.\nNo, Kirsten Gillibrand was born in Albany, New York on December 9, 1966.\nYes, Mario Cuomo was born in New York City, New York, United States. He was born on June 15, 1932, in the borough of Queens, specifically in the neighborhood of South Jamaica.\nAs of September 2021, Andrew Cuomo is the current Governor of New York. However, please note that political positions can change, and it is always recommended to verify the information with up-to-date sources.\n</pre> In\u00a0[5]: Copied! <pre># 4. Generate Final Verified Response\n# Given the discovered inconsistencies (if any), generate a revised response\n# incorporating the verification results.\n\ncontext = \"\\n\".join(verification_answers)\nverified_response = await answer_query_with_context(query, context)\nprint(verified_response)\n</pre> # 4. Generate Final Verified Response # Given the discovered inconsistencies (if any), generate a revised response # incorporating the verification results.  context = \"\\n\".join(verification_answers) verified_response = await answer_query_with_context(query, context) print(verified_response) <pre>Some politicians who were born in New York, New York include Franklin D. Roosevelt, Theodore Roosevelt, Donald Trump, Rudy Giuliani, and Mario Cuomo.\n</pre>"},{"location":"examples/chain_of_verification/#chain-of-verification-cove","title":"Chain of Verification (CoVe)\u00b6","text":"<p>This notebook is a basic implementation of the paper Chain-of-Verification Reduces Hallucination In Large Language Models (2023) (arXiv: [2309.11495]).</p>"},{"location":"examples/registering_custom_type/","title":"Registering a Custom Type","text":"In\u00a0[1]: Copied! <pre># Create FunctionSchema for pd.DataFrame\n\nimport json\nfrom typing import Any, Iterable\n\nimport pandas as pd\n\nfrom magentic.chat_model.function_schema import FunctionSchema, register_function_schema\n\n\n@register_function_schema(pd.DataFrame)\nclass DataFrameFunctionSchema(FunctionSchema[pd.DataFrame]):\n    @property\n    def name(self) -&gt; str:\n        \"\"\"The name of the function.\n\n        Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.\n        \"\"\"\n        return \"dataframe\"\n\n    @property\n    def description(self) -&gt; str | None:\n        return \"A DataFrame object.\"\n\n    @property\n    def parameters(self) -&gt; dict[str, Any]:\n        \"\"\"The parameters the functions accepts as a JSON Schema object.\"\"\"\n        return {\n            \"properties\": {\n                \"columns\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n                \"data\": {\n                    \"type\": \"array\",\n                    \"items\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n                },\n            },\n            \"required\": [\"index\", \"columns\", \"data\"],\n            \"type\": \"object\",\n        }\n\n    def parse_args(self, chunks: Iterable[str]) -&gt; pd.DataFrame:\n        \"\"\"Parse an iterable of string chunks into the function arguments.\"\"\"\n        args = json.loads(\"\".join(chunks))\n        return pd.DataFrame(**args)\n\n    def serialize_args(self, value: pd.DataFrame) -&gt; dict:\n        \"\"\"Serialize an object into a JSON string of function arguments.\"\"\"\n        return {\n            \"columns\": value.columns.tolist(),\n            \"data\": value.to_numpy().tolist(),\n        }\n</pre> # Create FunctionSchema for pd.DataFrame  import json from typing import Any, Iterable  import pandas as pd  from magentic.chat_model.function_schema import FunctionSchema, register_function_schema   @register_function_schema(pd.DataFrame) class DataFrameFunctionSchema(FunctionSchema[pd.DataFrame]):     @property     def name(self) -&gt; str:         \"\"\"The name of the function.          Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.         \"\"\"         return \"dataframe\"      @property     def description(self) -&gt; str | None:         return \"A DataFrame object.\"      @property     def parameters(self) -&gt; dict[str, Any]:         \"\"\"The parameters the functions accepts as a JSON Schema object.\"\"\"         return {             \"properties\": {                 \"columns\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},                 \"data\": {                     \"type\": \"array\",                     \"items\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},                 },             },             \"required\": [\"index\", \"columns\", \"data\"],             \"type\": \"object\",         }      def parse_args(self, chunks: Iterable[str]) -&gt; pd.DataFrame:         \"\"\"Parse an iterable of string chunks into the function arguments.\"\"\"         args = json.loads(\"\".join(chunks))         return pd.DataFrame(**args)      def serialize_args(self, value: pd.DataFrame) -&gt; dict:         \"\"\"Serialize an object into a JSON string of function arguments.\"\"\"         return {             \"columns\": value.columns.tolist(),             \"data\": value.to_numpy().tolist(),         } In\u00a0[2]: Copied! <pre># Roundtrip test the new FunctionSchema\n\nfunction_schema = DataFrameFunctionSchema(pd.DataFrame)\n\ndf_test = pd.DataFrame(\n    {\n        \"A\": [1, 2, 3],\n        \"B\": [4, 5, 6],\n    },\n)\n\nargs = function_schema.serialize_args(df_test)\nprint(args)\n\nobj = function_schema.parse_args(json.dumps(args))\nobj\n</pre> # Roundtrip test the new FunctionSchema  function_schema = DataFrameFunctionSchema(pd.DataFrame)  df_test = pd.DataFrame(     {         \"A\": [1, 2, 3],         \"B\": [4, 5, 6],     }, )  args = function_schema.serialize_args(df_test) print(args)  obj = function_schema.parse_args(json.dumps(args)) obj <pre>{'columns': ['A', 'B'], 'data': [[1, 4], [2, 5], [3, 6]]}\n</pre> Out[2]: A B 0 1 4 1 2 5 2 3 6 In\u00a0[3]: Copied! <pre># Use pd.DataFrame as the return type of a prompt function\n\nimport pandas as pd\n\nfrom magentic import prompt\n\n\n@prompt(\n    \"Create a table listing the ingredients needed to cook {dish}.\"\n    \"Include a column for the quantity of each ingredient.\"\n    \"Also include a column with alergy information.\"\n)\ndef list_ingredients(dish: str) -&gt; pd.DataFrame:\n    ...\n\n\nlist_ingredients(\"lasagna\")\n</pre> # Use pd.DataFrame as the return type of a prompt function  import pandas as pd  from magentic import prompt   @prompt(     \"Create a table listing the ingredients needed to cook {dish}.\"     \"Include a column for the quantity of each ingredient.\"     \"Also include a column with alergy information.\" ) def list_ingredients(dish: str) -&gt; pd.DataFrame:     ...   list_ingredients(\"lasagna\") Out[3]: Ingredient Quantity Allergy Information 0 Lasagna noodles 16 oz Contains wheat, may contain egg and soy 1 Ground beef 1 lb Contains beef, may contain soy and gluten 2 Tomato sauce 24 oz Contains tomatoes, may contain soy and garlic 3 Mozzarella cheese 16 oz Contains milk, may contain soy 4 Ricotta cheese 15 oz Contains milk, may contain soy and eggs 5 Parmesan cheese 1 cup Contains milk, may contain soy and eggs 6 Garlic 3 cloves No known allergies 7 Onion 1 No known allergies 8 Olive oil 2 tbsp No known allergies 9 Salt 1 tsp No known allergies 10 Pepper 1/2 tsp No known allergies 11 Italian seasoning 1 tsp No known allergies 12 Sugar 1 tsp No known allergies"},{"location":"examples/registering_custom_type/#registering-a-custom-type","title":"Registering a Custom Type\u00b6","text":"<p>This notebook shows how to register a new type so that it can be used as the return annotation for <code>@prompt</code>, <code>@promptchain</code>, and <code>@chatprompt</code>. This is done by creating a new <code>FunctionSchema</code> which defines the parameters required to create the type, and how to parse/serialize these from/to the LLM.</p> <p>See https://platform.openai.com/docs/guides/function-calling for more information on function calling, which enables this.</p>"},{"location":"examples/retrieval_augmented_generation/","title":"Retrieval Augmented Generation","text":"In\u00a0[1]: Copied! <pre>import wikipedia\n\nfrom magentic import prompt, prompt_chain\n</pre> import wikipedia  from magentic import prompt, prompt_chain <p>Define a standard prompt for explaining a topic.</p> In\u00a0[2]: Copied! <pre>@prompt(\n    \"Tell me about {topic} in simple terms. If you do not know, just say 'I do not know'.\"\n)\ndef explain_standard(topic: str) -&gt; str:\n    ...\n</pre> @prompt(     \"Tell me about {topic} in simple terms. If you do not know, just say 'I do not know'.\" ) def explain_standard(topic: str) -&gt; str:     ... <p>The model can explain topics that were in the training data (e.g. Treaty of Versailles), but it cannot explain (or fabricates an explanation for) topics that were not in the training data (e.g. Wagner Coup).</p> In\u00a0[3]: Copied! <pre>explain_standard(\"Treaty of Versailles\")\n</pre> explain_standard(\"Treaty of Versailles\") Out[3]: <pre>'The Treaty of Versailles was a peace agreement signed in 1919 after World War I. It was meant to decide what would happen to the countries involved in the war. Germany, which was blamed for starting the war, had to accept full responsibility and pay a lot of money as reparations. They also had to give up some of their land and reduce their military. Many people in Germany were unhappy with the treaty, and some believe it contributed to the start of World War II.'</pre> In\u00a0[4]: Copied! <pre>explain_standard(\"Wagner Coup\")\n</pre> explain_standard(\"Wagner Coup\") Out[4]: <pre>'I\\'m sorry, but I do not have any information about a \"Wagner Coup\" in my database. It\\'s possible that you may be referring to something specific that I\\'m not aware of.'</pre> <p>Wikipedia has up-to-date information on most topics so it can be used to provide context to the LLM.</p> In\u00a0[5]: Copied! <pre>wikipedia.summary(\"Wagner Coup\")\n</pre> wikipedia.summary(\"Wagner Coup\") Out[5]: <pre>'On 23 June 2023, the Wagner Group, a Russian government-funded paramilitary and private military company, staged a rebellion after a period of increasing tensions between the Russian Ministry of Defence and the leader of Wagner, Yevgeny Prigozhin.\\nWhile Prigozhin was supportive of the Russian invasion of Ukraine, he had previously publicly criticized Defense Minister Sergei Shoigu and Chief of the General Staff Valery Gerasimov, blaming them for the country\\'s military shortcomings and accusing them of handing over \"Russian territories\" to Ukraine. Prigozhin portrayed the rebellion as a response to an alleged attack on his forces by the ministry, and demanded that Shoigu and Gerasimov be turned over to him. In a televised address on 24 June, Russian president Vladimir Putin denounced Wagner\\'s actions as treason and pledged to quell the rebellion.\\nPrigozhin\\'s forces took control of Rostov-on-Don and the headquarters of the Southern Military District in the city. An armored column of Wagner troops advanced through Voronezh Oblast towards Moscow. Armed with mobile anti-aircraft systems, the rebels repelled the air attacks of the Russian military, whose actions did not deter the progress of the column. Ground defenses were concentrated on the approach to Moscow. Before Wagner reached the defenses, Belarusian president Alexander Lukashenko brokered a settlement with Prigozhin, who agreed to end the rebellion. On the late evening of 24 June, Wagner forces turned around, and those that had remained in Rostov-on-Don began withdrawing.\\nAs per the agreement, the Federal Security Service, which had initiated a case for armed rebellion under Article 279 of the Criminal Code closed the case on 27 June 2023, dropping the charges. At least thirteen servicemen of the Russian military were killed during the rebellion. On the rebels\\' side, several Wagner members were reported injured and two military defectors were killed according to Prigozhin.'</pre> <p>If the topics we receive are always present on Wikipedia then it might make sense to always query wikipedia and add this information to the context.</p> In\u00a0[6]: Copied! <pre>@prompt(\n    \"\"\"Based on the following context, tell me about {topic} in simple terms.\nIf you do not know, just say 'I do not know'.\n\ncontext\n---\n{context}\n\"\"\"\n)\ndef explain_using_context(topic: str, context) -&gt; str:\n    ...\n\n\ndef explain_using_wikipedia_context(topic: str) -&gt; str:\n    context = wikipedia.summary(topic)\n    return explain_using_context(topic, context=context)\n\n\nexplain_using_wikipedia_context(\"Wagner Coup\")\n</pre> @prompt(     \"\"\"Based on the following context, tell me about {topic} in simple terms. If you do not know, just say 'I do not know'.  context --- {context} \"\"\" ) def explain_using_context(topic: str, context) -&gt; str:     ...   def explain_using_wikipedia_context(topic: str) -&gt; str:     context = wikipedia.summary(topic)     return explain_using_context(topic, context=context)   explain_using_wikipedia_context(\"Wagner Coup\") Out[6]: <pre>\"The Wagner Coup was a rebellion that happened in Russia in June 2023. The Wagner Group, a paramilitary and private military company funded by the Russian government, staged the rebellion. The leader of Wagner, Yevgeny Prigozhin, was unhappy with the Russian Ministry of Defence and publicly criticized them. He accused them of causing military problems and giving away Russian territories to Ukraine. Prigozhin's forces took control of Rostov-on-Don and the headquarters of the Southern Military District. They also advanced towards Moscow with an armored column. The Russian military tried to stop them with air attacks, but the rebels had anti-aircraft systems and were able to repel the attacks. Before reaching Moscow, a settlement was brokered by the Belarusian president, and Prigozhin agreed to end the rebellion. The case of armed rebellion against Prigozhin was closed, and the charges were dropped. Several people were killed and injured during the rebellion.\"</pre> <p>Alternatively, if we only need to query Wikipedia in some cases then we can provide a <code>wikipedia_summary</code> function for the LLM to use as needed. This will avoid querying Wikipedia in some cases. When Wikipedia is queried two calls will be made to the LLM (one that decided to query wikipedia, and one to generate the explanation from the wikipedia summary).</p> In\u00a0[7]: Copied! <pre>def wikipedia_summary(topic: str) -&gt; str:\n    print(f\" - Searching Wikipedia for {topic!r}\")\n    return wikipedia.summary(topic)\n\n\n@prompt_chain(\n    \"Tell me about {topic} in simple terms. Only query wikipedia if you do not already know about {topic}.\",\n    functions=[wikipedia_summary],\n)\ndef explain_using_function_calling(topic: str) -&gt; str:\n    ...\n\n\nexplain_using_function_calling(\"Wagner Coup\")\n</pre> def wikipedia_summary(topic: str) -&gt; str:     print(f\" - Searching Wikipedia for {topic!r}\")     return wikipedia.summary(topic)   @prompt_chain(     \"Tell me about {topic} in simple terms. Only query wikipedia if you do not already know about {topic}.\",     functions=[wikipedia_summary], ) def explain_using_function_calling(topic: str) -&gt; str:     ...   explain_using_function_calling(\"Wagner Coup\") <pre> - Searching Wikipedia for 'Wagner Coup'\n</pre> Out[7]: <pre>'The Wagner Coup was a rebellion that took place on June 23, 2023, led by the Wagner Group, a Russian government-funded paramilitary and private military company. The coup was sparked by tensions between the Russian Ministry of Defence and the leader of Wagner, Yevgeny Prigozhin.\\n\\nPrigozhin, who supported the Russian invasion of Ukraine, had criticized Defense Minister Sergei Shoigu and Chief of the General Staff Valery Gerasimov for the country\\'s military shortcomings. He accused them of giving away \"Russian territories\" to Ukraine. Prigozhin claimed that the rebellion was in response to an alleged attack on his forces by the ministry and demanded that Shoigu and Gerasimov be handed over to him.\\n\\nRussian President Vladimir Putin denounced Wagner\\'s actions as treason and vowed to suppress the rebellion. Prigozhin\\'s forces took control of Rostov-on-Don and the headquarters of the Southern Military District. They also advanced towards Moscow with an armored column, repelling air attacks from the Russian military.\\n\\nBefore reaching Moscow\\'s defenses, a settlement was brokered by Belarusian President Alexander Lukashenko. Prigozhin agreed to end the rebellion, and Wagner forces began withdrawing. The Federal Security Service closed the case on armed rebellion, dropping the charges.\\n\\nDuring the coup, at least thirteen Russian military servicemen were killed, while several Wagner members were reported injured and two military defectors were killed according to Prigozhin.'</pre> <p>Despite the function call being optional the LLM might decide to use it when it doesn't need to!</p> In\u00a0[8]: Copied! <pre>explain_using_function_calling(\"Treaty of Versailles\")\n</pre> explain_using_function_calling(\"Treaty of Versailles\") <pre> - Searching Wikipedia for 'Treaty of Versailles'\n</pre> Out[8]: <pre>\"The Treaty of Versailles was a peace treaty signed in 1919 to end World War I. It was signed in the Palace of Versailles, France. The treaty was important because it ended the state of war between Germany and most of the Allied Powers. Germany was blamed for causing the war and was required to accept responsibility for the damage caused. The treaty also required Germany to disarm, give up territory, and pay reparations to the countries that had been affected by the war. The treaty was controversial because some people thought it was too harsh on Germany, while others thought it was too lenient. The treaty's terms had long-lasting effects, including economic collapse in Germany and the rise of the Nazi Party, which eventually led to World War II.\"</pre> <p>One way to combat this is to create a simple explain function that returns <code>None</code> for unknown answers, then explicitly check whether the model was able to respond without the wikipedia context, and call the LLM with the wikipedia context if not. Essentially we take some responsibility away from the LLM and put it in code instead. This approach makes most sense if the function for fetching context is slow or expensive, or should only be needed infrequently.</p> In\u00a0[9]: Copied! <pre>@prompt(\"Tell me about {topic} in simple terms. If you do not know, return null.\")\ndef explain_or_none(topic: str) -&gt; str | None:\n    ...\n\n\ndef explain_final(topic: str) -&gt; str:\n    explanation = explain_or_none(topic)\n    if not explanation:\n        print(\" - Received null response\")\n        return explain_using_wikipedia_context(topic)\n    return explanation\n\n\nexplain_final(\"Wagner Coup\")\n</pre> @prompt(\"Tell me about {topic} in simple terms. If you do not know, return null.\") def explain_or_none(topic: str) -&gt; str | None:     ...   def explain_final(topic: str) -&gt; str:     explanation = explain_or_none(topic)     if not explanation:         print(\" - Received null response\")         return explain_using_wikipedia_context(topic)     return explanation   explain_final(\"Wagner Coup\") <pre> - Received null response\n</pre> Out[9]: <pre>\"The Wagner Coup was a rebellion that happened in Russia in June 2023. The Wagner Group, a paramilitary and private military company funded by the Russian government, staged the rebellion. The leader of Wagner, Yevgeny Prigozhin, was unhappy with the Russian Ministry of Defence and publicly criticized them. He accused them of causing military problems and giving away Russian territories to Ukraine. Prigozhin's forces took control of Rostov-on-Don and the headquarters of the Southern Military District. They also advanced towards Moscow with an armored column. The Russian military tried to stop them, but the rebels had anti-aircraft systems that repelled the attacks. Before reaching Moscow, a settlement was brokered by the Belarusian president, and Prigozhin agreed to end the rebellion. The case of armed rebellion against the rebels was closed, and several people were killed or injured during the conflict.\"</pre> In\u00a0[10]: Copied! <pre>explain_final(\"Treaty of Versailles\")\n</pre> explain_final(\"Treaty of Versailles\") Out[10]: <pre>\"The Treaty of Versailles was a peace treaty signed after World War I. It was signed in 1919 in the Palace of Versailles in France. The treaty placed the blame for the war on Germany and its allies. It imposed heavy reparations on Germany, meaning they had to pay a lot of money to the countries that were affected by the war. The treaty also limited the size of Germany's military and took away some of its territory. Many people believe that the harsh terms of the treaty contributed to the rise of Adolf Hitler and the start of World War II.\"</pre>"},{"location":"examples/retrieval_augmented_generation/#retrieval-augmented-generation","title":"Retrieval Augmented Generation\u00b6","text":"<p>This notebook shows how to perform Retrieval Augmented Generation (RAG) using <code>magentic</code> and the <code>wikipedia</code> API. Essentially providing context to the LLM which it can use when generating its response. This approach allows us to insert new or private information that was not present in the model's training data. The Wikipedia API is used here for demonstration but the methods shown are applicable to any data source.</p>"}]}