{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>Seamlessly integrate Large Language Models into Python code. Use the <code>@prompt</code> and <code>@chatprompt</code> decorators to create functions that return structured output from an LLM. Combine LLM queries and tool use with traditional Python code to build complex agentic systems.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Structured Outputs using pydantic models and built-in python types.</li> <li>Streaming of structured outputs and function calls, to use them while being generated.</li> <li>LLM-Assisted Retries to improve LLM adherence to complex output schemas.</li> <li>Observability using OpenTelemetry, with native Pydantic Logfire integration.</li> <li>Type Annotations to work nicely with linters and IDEs.</li> <li>Configuration options for multiple LLM providers including OpenAI, Anthropic, and Ollama.</li> <li>Many more features: Chat Prompting, Parallel Function Calling, Vision, Formatting, Asyncio...</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install magentic\n</code></pre> <p>or using uv</p> <pre><code>uv add magentic\n</code></pre> <p>Configure your OpenAI API key by setting the <code>OPENAI_API_KEY</code> environment variable. To configure a different LLM provider see Configuration for more.</p>"},{"location":"#usage","title":"Usage","text":""},{"location":"#prompt","title":"@prompt","text":"<p>The <code>@prompt</code> decorator allows you to define a template for a Large Language Model (LLM) prompt as a Python function. When this function is called, the arguments are inserted into the template, then this prompt is sent to an LLM which generates the function output.</p> <pre><code>from magentic import prompt\n\n\n@prompt('Add more \"dude\"ness to: {phrase}')\ndef dudeify(phrase: str) -&gt; str: ...  # No function body as this is never executed\n\n\ndudeify(\"Hello, how are you?\")\n# \"Hey, dude! What's up? How's it going, my man?\"\n</code></pre> <p>The <code>@prompt</code> decorator will respect the return type annotation of the decorated function. This can be any type supported by pydantic including a <code>pydantic</code> model.</p> <pre><code>from magentic import prompt\nfrom pydantic import BaseModel\n\n\nclass Superhero(BaseModel):\n    name: str\n    age: int\n    power: str\n    enemies: list[str]\n\n\n@prompt(\"Create a Superhero named {name}.\")\ndef create_superhero(name: str) -&gt; Superhero: ...\n\n\ncreate_superhero(\"Garden Man\")\n# Superhero(name='Garden Man', age=30, power='Control over plants', enemies=['Pollution Man', 'Concrete Woman'])\n</code></pre> <p>See Structured Outputs for more.</p>"},{"location":"#chatprompt","title":"@chatprompt","text":"<p>The <code>@chatprompt</code> decorator works just like <code>@prompt</code> but allows you to pass chat messages as a template rather than a single text prompt. This can be used to provide a system message or for few-shot prompting where you provide example responses to guide the model's output. Format fields denoted by curly braces <code>{example}</code> will be filled in all messages (except <code>FunctionResultMessage</code>).</p> <pre><code>from magentic import chatprompt, AssistantMessage, SystemMessage, UserMessage\nfrom pydantic import BaseModel\n\n\nclass Quote(BaseModel):\n    quote: str\n    character: str\n\n\n@chatprompt(\n    SystemMessage(\"You are a movie buff.\"),\n    UserMessage(\"What is your favorite quote from Harry Potter?\"),\n    AssistantMessage(\n        Quote(\n            quote=\"It does not do to dwell on dreams and forget to live.\",\n            character=\"Albus Dumbledore\",\n        )\n    ),\n    UserMessage(\"What is your favorite quote from {movie}?\"),\n)\ndef get_movie_quote(movie: str) -&gt; Quote: ...\n\n\nget_movie_quote(\"Iron Man\")\n# Quote(quote='I am Iron Man.', character='Tony Stark')\n</code></pre> <p>See Chat Prompting for more.</p>"},{"location":"#functioncall","title":"FunctionCall","text":"<p>An LLM can also decide to call functions. In this case the <code>@prompt</code>-decorated function returns a <code>FunctionCall</code> object which can be called to execute the function using the arguments provided by the LLM.</p> <pre><code>from typing import Literal\n\nfrom magentic import prompt, FunctionCall\n\n\ndef search_twitter(query: str, category: Literal[\"latest\", \"people\"]) -&gt; str:\n    \"\"\"Searches Twitter for a query.\"\"\"\n    print(f\"Searching Twitter for {query!r} in category {category!r}\")\n    return \"&lt;twitter results&gt;\"\n\n\ndef search_youtube(query: str, channel: str = \"all\") -&gt; str:\n    \"\"\"Searches YouTube for a query.\"\"\"\n    print(f\"Searching YouTube for {query!r} in channel {channel!r}\")\n    return \"&lt;youtube results&gt;\"\n\n\n@prompt(\n    \"Use the appropriate search function to answer: {question}\",\n    functions=[search_twitter, search_youtube],\n)\ndef perform_search(question: str) -&gt; FunctionCall[str]: ...\n\n\noutput = perform_search(\"What is the latest news on LLMs?\")\nprint(output)\n# &gt; FunctionCall(&lt;function search_twitter at 0x10c367d00&gt;, 'LLMs', 'latest')\noutput()\n# &gt; Searching Twitter for 'Large Language Models news' in category 'latest'\n# '&lt;twitter results&gt;'\n</code></pre> <p>See Function Calling for more.</p>"},{"location":"#prompt_chain","title":"@prompt_chain","text":"<p>Sometimes the LLM requires making one or more function calls to generate a final answer. The <code>@prompt_chain</code> decorator will resolve <code>FunctionCall</code> objects automatically and pass the output back to the LLM to continue until the final answer is reached.</p> <p>In the following example, when <code>describe_weather</code> is called the LLM first calls the <code>get_current_weather</code> function, then uses the result of this to formulate its final answer which gets returned.</p> <pre><code>from magentic import prompt_chain\n\n\ndef get_current_weather(location, unit=\"fahrenheit\"):\n    \"\"\"Get the current weather in a given location\"\"\"\n    # Pretend to query an API\n    return {\"temperature\": \"72\", \"forecast\": [\"sunny\", \"windy\"]}\n\n\n@prompt_chain(\n    \"What's the weather like in {city}?\",\n    functions=[get_current_weather],\n)\ndef describe_weather(city: str) -&gt; str: ...\n\n\ndescribe_weather(\"Boston\")\n# 'The current weather in Boston is 72\u00b0F and it is sunny and windy.'\n</code></pre> <p>LLM-powered functions created using <code>@prompt</code>, <code>@chatprompt</code> and <code>@prompt_chain</code> can be supplied as <code>functions</code> to other <code>@prompt</code>/<code>@prompt_chain</code> decorators, just like regular python functions. This enables increasingly complex LLM-powered functionality, while allowing individual components to be tested and improved in isolation.</p>"},{"location":"asyncio/","title":"Asyncio","text":"<p>Asynchronous functions / coroutines can be used to concurrently query the LLM. This can greatly increase the overall speed of generation, and also allow other asynchronous code to run while waiting on LLM output. In the below example, the LLM generates a description for each US president while it is waiting on the next one in the list. Measuring the characters generated per second shows that this example achieves a 7x speedup over serial processing.</p> <pre><code>import asyncio\nfrom time import time\nfrom typing import AsyncIterable\n\nfrom magentic import prompt\n\n\n@prompt(\"List ten presidents of the United States\")\nasync def iter_presidents() -&gt; AsyncIterable[str]: ...\n\n\n@prompt(\"Tell me more about {topic}\")\nasync def tell_me_more_about(topic: str) -&gt; str: ...\n\n\n# For each president listed, generate a description concurrently\nstart_time = time()\ntasks = []\nasync for president in await iter_presidents():\n    # Use asyncio.create_task to schedule the coroutine for execution before awaiting it\n    # This way descriptions will start being generated while the list of presidents is still being generated\n    task = asyncio.create_task(tell_me_more_about(president))\n    tasks.append(task)\n\ndescriptions = await asyncio.gather(*tasks)\n\n# Measure the characters per second\ntotal_chars = sum(len(desc) for desc in descriptions)\ntime_elapsed = time() - start_time\nprint(total_chars, time_elapsed, total_chars / time_elapsed)\n# 24575 28.70 856.07\n\n\n# Measure the characters per second to describe a single president\nstart_time = time()\nout = await tell_me_more_about(\"George Washington\")\ntime_elapsed = time() - start_time\nprint(len(out), time_elapsed, len(out) / time_elapsed)\n# 2206 18.72 117.78\n</code></pre>"},{"location":"chat-prompting/","title":"Chat Prompting","text":"<p>This page covers the <code>@chatprompt</code> decorator which can be used to define an LLM query template (message templates, optional LLM, functions, and return type). To manage an ongoing conversation with an LLM, see Chat.</p>"},{"location":"chat-prompting/#chatprompt","title":"@chatprompt","text":"<p>The <code>@chatprompt</code> decorator works just like <code>@prompt</code> but allows you to pass chat messages as a template rather than a single text prompt. This can be used to provide a system message or for few-shot prompting where you provide example responses to guide the model's output. Format fields denoted by curly braces <code>{example}</code> will be filled in all messages (except <code>FunctionResultMessage</code>).</p> <pre><code>from magentic import chatprompt, AssistantMessage, SystemMessage, UserMessage\nfrom pydantic import BaseModel\n\n\nclass Quote(BaseModel):\n    quote: str\n    character: str\n\n\n@chatprompt(\n    SystemMessage(\"You are a movie buff.\"),\n    UserMessage(\"What is your favorite quote from Harry Potter?\"),\n    AssistantMessage(\n        Quote(\n            quote=\"It does not do to dwell on dreams and forget to live.\",\n            character=\"Albus Dumbledore\",\n        )\n    ),\n    UserMessage(\"What is your favorite quote from {movie}?\"),\n)\ndef get_movie_quote(movie: str) -&gt; Quote: ...\n\n\nget_movie_quote(\"Iron Man\")\n# Quote(quote='I am Iron Man.', character='Tony Stark')\n</code></pre>"},{"location":"chat-prompting/#escape_braces","title":"escape_braces","text":"<p>To prevent curly braces from being interpreted as format fields, use the <code>escape_braces</code> function to escape them in strings.</p> <pre><code>from magentic.chatprompt import escape_braces\n\nstring_with_braces = \"Curly braces like {example} will be filled in!\"\nescaped_string = escape_braces(string_with_braces)\n# 'Curly braces {{example}} will be filled in!'\nescaped_string.format(example=\"test\")\n# 'Curly braces {example} will be filled in!'\n</code></pre>"},{"location":"chat-prompting/#placeholder","title":"Placeholder","text":"<p>The <code>Placeholder</code> class enables templating of message content within the <code>@chatprompt</code> decorator. This allows dynamic changing of the messages used to prompt the model based on the arguments provided when the function is called.</p> <pre><code>from magentic import chatprompt, AssistantMessage, Placeholder, UserMessage\nfrom pydantic import BaseModel\n\n\nclass Quote(BaseModel):\n    quote: str\n    character: str\n\n\n@chatprompt(\n    UserMessage(\"Tell me a quote from {movie}\"),\n    AssistantMessage(Placeholder(Quote, \"quote\")),\n    UserMessage(\"What is a similar quote from the same movie?\"),\n)\ndef get_similar_quote(movie: str, quote: Quote) -&gt; Quote: ...\n\n\nget_similar_quote(\n    movie=\"Star Wars\",\n    quote=Quote(quote=\"I am your father\", character=\"Darth Vader\"),\n)\n# Quote(quote='The Force will be with you, always.', character='Obi-Wan Kenobi')\n</code></pre> <p><code>Placeholder</code> can also be used in <code>UserMessage</code> to allow inserting <code>ImageBytes</code>, <code>ImageUrl</code>, or other content blocks from function arguments. For more information see Vision.</p>"},{"location":"chat-prompting/#functioncall","title":"FunctionCall","text":"<p>The content of an <code>AssistantMessage</code> can be a <code>FunctionCall</code>. This can be used to demonstrate to the LLM when/how it should call a function.</p> <pre><code>from magentic import (\n    chatprompt,\n    AssistantMessage,\n    FunctionCall,\n    UserMessage,\n    SystemMessage,\n)\n\n\ndef change_music_volume(increment: int) -&gt; int:\n    \"\"\"Change music volume level. Min 1, max 10.\"\"\"\n    print(f\"Music volume change: {increment}\")\n\n\ndef order_food(food: str, amount: int):\n    \"\"\"Order food.\"\"\"\n    print(f\"Ordered {amount} {food}\")\n\n\n@chatprompt(\n    SystemMessage(\n        \"You are hosting a party and must keep the guests happy.\"\n        \"Call functions as needed. Do not respond directly.\"\n    ),\n    UserMessage(\"It's pretty loud in here!\"),\n    AssistantMessage(FunctionCall(change_music_volume, -2)),\n    UserMessage(\"{request}\"),\n    functions=[change_music_volume, order_food],\n)\ndef adjust_for_guest(request: str) -&gt; FunctionCall[None]: ...\n\n\nfunc = adjust_for_guest(\"Do you have any more food?\")\nfunc()\n# Ordered 3 pizza\n</code></pre>"},{"location":"chat-prompting/#functionresultmessage","title":"FunctionResultMessage","text":"<p>To include the result of calling the function in the messages use a <code>FunctionResultMessage</code>. This takes a <code>FunctionCall</code> instance as its second argument. The same <code>FunctionCall</code> instance must be passed to an <code>AssistantMessage</code> and the corresponding <code>FunctionResultMessage</code> so that the result can be correctly linked back to the function call that created it.</p> <pre><code>from magentic import (\n    chatprompt,\n    AssistantMessage,\n    FunctionCall,\n    FunctionResultMessage,\n    UserMessage,\n)\n\n\ndef plus(a: int, b: int) -&gt; int:\n    return a + b\n\n\nplus_1_2 = FunctionCall(plus, 1, 2)\n\n\n@chatprompt(\n    UserMessage(\"Use the plus function to add 1 and 2.\"),\n    AssistantMessage(plus_1_2),\n    FunctionResultMessage(3, plus_1_2),\n    UserMessage(\"Now add 4 to the result.\"),\n    functions=[plus],\n)\ndef do_math() -&gt; FunctionCall[int]: ...\n\n\ndo_math()\n# FunctionCall(&lt;function plus at 0x10a0829e0&gt;, 3, 4)\n</code></pre>"},{"location":"chat-prompting/#anymessage","title":"AnyMessage","text":"<p>The <code>AnyMessage</code> type can be used for (de)serialization of <code>Message</code> objects, or as a return type in prompt-functions. This allows you to create prompt-functions to do things like summarize a chat history into fewer messages, or even to create a set of messages that you can use in a chatprompt-function.</p> <pre><code>from magentic import AnyMessage, prompt\n\n\n@prompt(\"Create an example of few-shot prompting for a chatbot\")\ndef make_few_shot_prompt() -&gt; list[AnyMessage]: ...\n\n\nmake_few_shot_prompt()\n# [SystemMessage('You are a helpful and knowledgeable assistant. You answer questions promptly and accurately. Always be polite and concise.'),\n#  UserMessage('What\u2019s the weather like today?'),\n#  AssistantMessage[Any]('The weather today is sunny with a high of 75\u00b0F (24\u00b0C) and a low of 55\u00b0F (13\u00b0C). No chance of rain.'),\n#  UserMessage('Can you explain the theory of relativity in simple terms?'),\n#  AssistantMessage[Any]('Sure! The theory of relativity, developed by Albert Einstein, has two main parts: Special Relativity and General Relativity. Special Relativity is about how time and space are linked for objects moving at a consistent speed in a straight line. It shows that time can slow down or speed up depending on how fast you are moving compared to something else. General Relativity adds gravity into the mix and shows that massive objects cause space to bend and warp, which we feel as gravity.'),\n#  UserMessage('How do I bake a chocolate cake?'),\n#  AssistantMessage[Any](\"Here's a simple recipe for a chocolate cake:\\n\\nIngredients:\\n- 1 and 3/4 cups all-purpose flour\\n- 1 and 1/2 cups granulated sugar\\n- 3/4 cup cocoa powder\\n- 1 and 1/2 teaspoons baking powder\\n- 1 and 1/2 teaspoons baking soda\\n- 1 teaspoon salt\\n- 2 large eggs\\n- 1 cup whole milk\\n- 1/2 cup vegetable oil\\n- 2 teaspoons vanilla extract\\n- 1 cup boiling water\\n\\nInstructions:\\n1. Preheat your oven to 350\u00b0F (175\u00b0C). Grease and flour two 9-inch round baking pans.\\n2. In a large bowl, whisk together the flour, sugar, cocoa powder, baking powder, baking soda, and salt.\\n3. Add the eggs, milk, oil, and vanilla. Beat on medium speed for 2 minutes.\\n4. Stir in the boiling water (batter will be thin).\\n5. Pour the batter evenly into the prepared pans.\\n6. Bake for 30 to 35 minutes or until a toothpick inserted into the center comes out clean.\\n7. Cool the cakes in the pans for 10 minutes, then remove them from the pans and cool completely on a wire rack.\\n8. Frost with your favorite chocolate frosting and enjoy!\")]\n</code></pre> <p>For (de)serialization, check out <code>TypeAdapter</code> from pydantic. See more on the pydantic Type Adapter docs page.</p> <pre><code>from magentic import AnyMessage\nfrom pydantic import TypeAdapter\n\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"Hello\"},\n    {\"role\": \"user\", \"content\": \"Hello\"},\n    {\"role\": \"assistant\", \"content\": \"Hello\"},\n    {\"role\": \"tool\", \"content\": 3, \"tool_call_id\": \"unique_id\"},\n]\nTypeAdapter(list[AnyMessage]).validate_python(messages)\n# [SystemMessage('Hello'),\n#  UserMessage('Hello'),\n#  AssistantMessage[Any]('Hello'),\n#  ToolResultMessage[Any](3, self.tool_call_id='unique_id')]\n</code></pre>"},{"location":"chat/","title":"Chat","text":"<p>This page covers the <code>Chat</code> class which can be used to manage an ongoing conversation with an LLM chat model. To define a reusable LLM query template use the <code>@chatprompt</code> decorator instead, see Chat Prompting.</p> <p>The <code>Chat</code> class represents an ongoing conversation with an LLM. It keeps track of the messages exchanged between the user and the model and allows submitting the conversation to the model to get a response.</p>"},{"location":"chat/#basic-usage","title":"Basic Usage","text":"<pre><code>from magentic import Chat, OpenaiChatModel, UserMessage\n\n# Create a new Chat instance\nchat = Chat(\n    messages=[UserMessage(\"Say hello\")],\n    model=OpenaiChatModel(\"gpt-4o\"),\n)\n\n# Append a new user message\nchat = chat.add_user_message(\"Actually, say goodbye!\")\nprint(chat.messages)\n# [UserMessage('Say hello'), UserMessage('Actually, say goodbye!')]\n\n# Submit the chat to the LLM to get a response\nchat = chat.submit()\nprint(chat.last_message.content)\n# 'Hello! Just kidding\u2014goodbye!'\n</code></pre> <p>Note that all methods of <code>Chat</code> return a new <code>Chat</code> instance with the updated messages. This allows branching the conversation and keeping track of multiple conversation paths.</p> <p>The following methods are available to manually add messages to the chat by providing just the content of the message:</p> <ul> <li><code>add_system_message</code>: Adds a system message to the chat.</li> <li><code>add_user_message</code>: Adds a user message to the chat.</li> <li><code>add_assistant_message</code>: Adds an assistant message to the chat.</li> </ul> <p>There is also a generic <code>add_message</code> method to add a <code>Message</code> object to the chat. And the <code>submit</code> method is used to submit the chat to the LLM model which adds an <code>AssistantMessage</code> to the chat containing the model's response.</p>"},{"location":"chat/#function-calling","title":"Function Calling","text":"<p>Function calling can be done with the <code>Chat</code> class by providing the list of functions when creating the instance, similar to the <code>@chatprompt</code> decorator. Similarly, structured outputs can be returned by setting the <code>output_types</code> parameter.</p> <p>If the last message in the chat is an <code>AssistantMessage</code> containing a <code>FunctionCall</code> or <code>ParallelFunctionCall</code>, calling the <code>exec_function_call</code> method will execute the function call(s) and append the result(s) to the chat. Then, if needed, the chat can be submitted to the LLM again to get another response.</p> <pre><code>from magentic import (\n    AssistantMessage,\n    Chat,\n    FunctionCall,\n    OpenaiChatModel,\n    UserMessage,\n)\n\n\ndef get_current_weather(location, unit=\"fahrenheit\"):\n    \"\"\"Get the current weather in a given location\"\"\"\n    # Pretend to query an API\n    return {\"temperature\": \"72\", \"forecast\": [\"sunny\", \"windy\"]}\n\n\nchat = Chat(\n    messages=[UserMessage(\"What's the weather like in Boston?\")],\n    functions=[get_current_weather],\n    # `FunctionCall` must be in output_types to get `FunctionCall` outputs\n    output_types=[FunctionCall, str],\n    model=OpenaiChatModel(\"gpt-4o\"),\n)\nchat = chat.submit()\nprint(chat.messages)\n# [UserMessage(\"What's the weather like in Boston?\"),\n#  AssistantMessage(FunctionCall(&lt;function get_current_weather at 0x130a92160&gt;, 'Boston'))]\n\n# Execute the function call and append the result to the chat\nchat = chat.exec_function_call()\nprint(chat.messages)\n# [UserMessage(\"What's the weather like in Boston?\"),\n#  AssistantMessage(FunctionCall(&lt;function get_current_weather at 0x130a92160&gt;, 'Boston')),\n#  FunctionResultMessage({'location': 'Boston', 'temperature': '72', 'unit': 'fahrenheit', 'forecast': ['sunny', 'windy']},\n#                        FunctionCall(&lt;function get_current_weather at 0x130a92160&gt;, 'Boston'))]\n\n# Submit the chat again to get the final LLM response\nchat = chat.submit()\nprint(chat.messages)\n# [UserMessage(\"What's the weather like in Boston?\"),\n#  AssistantMessage(FunctionCall(&lt;function get_current_weather at 0x130a92160&gt;, 'Boston')),\n#  FunctionResultMessage({'location': 'Boston', 'temperature': '72', 'unit': 'fahrenheit', 'forecast': ['sunny', 'windy']},\n#                        FunctionCall(&lt;function get_current_weather at 0x130a92160&gt;, 'Boston')),\n#  AssistantMessage(\"The current weather in Boston is 72\u00b0F, and it's sunny with windy conditions.\")]\n</code></pre>"},{"location":"chat/#streaming","title":"Streaming","text":"<p>Streaming types such as <code>StreamedStr</code>, <code>StreamedOutput</code>, and <code>Iterable[T]</code> can be provided in the <code>output_types</code> parameter. When the <code>.submit()</code> method is called, an <code>AssistantMessage</code> containing the streamed type will be appended to the chat immediately. This allows the streamed type to be accessed and streamed from. For more information on streaming types, see Streaming.</p> <pre><code>from magentic import Chat, UserMessage, StreamedStr\n\nchat = Chat(\n    messages=[UserMessage(\"Tell me about the Golden Gate Bridge.\")],\n    output_types=[StreamedStr],\n)\nchat = chat.submit()\nprint(type(chat.last_message.content))\n# &lt;class 'magentic.streaming.StreamedStr'&gt;\n\nfor chunk in chat.last_message.content:\n    print(chunk, end=\"\")\n# (streamed) 'The Golden Gate Bridge is an iconic suspension bridge...\n</code></pre>"},{"location":"chat/#asyncio","title":"Asyncio","text":"<p>The <code>Chat</code> class also support asynchronous usage through the following methods:</p> <ul> <li><code>asubmit</code>: Asynchronously submit the chat to the LLM model.</li> <li><code>aexec_function_call</code>: Asynchronously execute the function call in the chat. This is required to handle the <code>AsyncParallelFunctionCall</code> output type.</li> </ul>"},{"location":"chat/#agent","title":"Agent","text":"<p>A very basic form of an agent can be created by running a loop that submits the chat to the LLM and executes function calls until some stop condition is met.</p> <pre><code>from magentic import Chat, FunctionCall, ParallelFunctionCall, UserMessage\n\n\ndef get_current_weather(location, unit=\"fahrenheit\"):\n    \"\"\"Get the current weather in a given location\"\"\"\n    # Pretend to query an API\n    return {\"temperature\": \"72\", \"forecast\": [\"sunny\", \"windy\"]}\n\n\nchat = Chat(\n    messages=[UserMessage(\"What's the weather like in Boston?\")],\n    functions=[get_current_weather],\n    output_types=[FunctionCall, str],\n).submit()\nwhile isinstance(chat.last_message.content, FunctionCall | ParallelFunctionCall):\n    chat = chat.exec_function_call().submit()\nprint(chat.last_message.content)\n# 'The current weather in Boston is 72\u00b0F, with sunny and windy conditions.'\n</code></pre>"},{"location":"configuration/","title":"LLM Configuration","text":""},{"location":"configuration/#backends","title":"Backends","text":"<p>Magentic supports multiple LLM providers or \"backends\". This roughly refers to which Python package is used to interact with the LLM API. The following backends are supported.</p>"},{"location":"configuration/#openai","title":"OpenAI","text":"<p>The default backend, using the <code>openai</code> Python package and supports all features of magentic.</p> <p>No additional installation is required. Just import the <code>OpenaiChatModel</code> class from <code>magentic</code>.</p> <pre><code>from magentic import OpenaiChatModel\n\nmodel = OpenaiChatModel(\"gpt-4o\")\n</code></pre>"},{"location":"configuration/#ollama-via-openai","title":"Ollama via OpenAI","text":"<p>Ollama supports an OpenAI-compatible API, which allows you to use Ollama models via the OpenAI backend.</p> <p>First, install ollama from ollama.com. Then, pull the model you want to use.</p> <pre><code>ollama pull llama3.2\n</code></pre> <p>Then, specify the model name and <code>base_url</code> when creating the <code>OpenaiChatModel</code> instance.</p> <pre><code>from magentic import OpenaiChatModel\n\nmodel = OpenaiChatModel(\"llama3.2\", base_url=\"http://localhost:11434/v1/\")\n</code></pre>"},{"location":"configuration/#other-openai-compatible-apis","title":"Other OpenAI-compatible APIs","text":"<p>When using the <code>openai</code> backend, setting the <code>MAGENTIC_OPENAI_BASE_URL</code> environment variable or using <code>OpenaiChatModel(..., base_url=\"http://localhost:8080\")</code> in code allows you to use <code>magentic</code> with any OpenAI-compatible API e.g. Azure OpenAI Service, LiteLLM OpenAI Proxy Server, LocalAI. Note that if the API does not support tool calls then you will not be able to create prompt-functions that return Python objects, but other features of <code>magentic</code> will still work.</p> <p>To use Azure with the openai backend you will need to set the <code>MAGENTIC_OPENAI_API_TYPE</code> environment variable to \"azure\" or use <code>OpenaiChatModel(..., api_type=\"azure\")</code>, and also set the environment variables needed by the openai package to access Azure. See https://github.com/openai/openai-python#microsoft-azure-openai</p>"},{"location":"configuration/#anthropic","title":"Anthropic","text":"<p>This uses the <code>anthropic</code> Python package and supports all features of magentic.</p> <p>Install the <code>magentic</code> package with the <code>anthropic</code> extra, or install the <code>anthropic</code> package directly.</p> <pre><code>pip install \"magentic[anthropic]\"\n</code></pre> <p>Then import the <code>AnthropicChatModel</code> class.</p> <pre><code>from magentic.chat_model.anthropic_chat_model import AnthropicChatModel\n\nmodel = AnthropicChatModel(\"claude-3-5-sonnet-latest\")\n</code></pre>"},{"location":"configuration/#litellm","title":"LiteLLM","text":"<p>This uses the <code>litellm</code> Python package to enable querying LLMs from many different providers. Note: some models may not support all features of <code>magentic</code> e.g. function calling/structured output and streaming.</p> <p>Install the <code>magentic</code> package with the <code>litellm</code> extra, or install the <code>litellm</code> package directly.</p> <pre><code>pip install \"magentic[litellm]\"\n</code></pre> <p>Then import the <code>LitellmChatModel</code> class.</p> <pre><code>from magentic.chat_model.litellm_chat_model import LitellmChatModel\n\nmodel = LitellmChatModel(\"gpt-4o\")\n</code></pre>"},{"location":"configuration/#mistral","title":"Mistral","text":"<p>This uses the <code>openai</code> Python package with some small modifications to make the API queries compatible with the Mistral API. It supports all features of magentic. However tool calls (including structured outputs) are not streamed so are received all at once.</p> <p>Note: a future version of magentic might switch to using the <code>mistral</code> Python package.</p> <p>No additional installation is required. Just import the <code>MistralChatModel</code> class.</p> <pre><code>from magentic.chat_model.mistral_chat_model import MistralChatModel\n\nmodel = MistralChatModel(\"mistral-large-latest\")\n</code></pre>"},{"location":"configuration/#configure-a-backend","title":"Configure a Backend","text":"<p>The default <code>ChatModel</code> used by <code>magentic</code> (in <code>@prompt</code>, <code>@chatprompt</code>, etc.) can be configured in several ways. When a prompt-function or chatprompt-function is called, the <code>ChatModel</code> to use follows this order of preference</p> <ol> <li>The <code>ChatModel</code> instance provided as the <code>model</code> argument to the magentic decorator</li> <li>The current chat model context, created using <code>with MyChatModel:</code></li> <li>The global <code>ChatModel</code> created from environment variables and the default settings in src/magentic/settings.py</li> </ol> <p>The following code snippet demonstrates this behavior:</p> <pre><code>from magentic import OpenaiChatModel, prompt\nfrom magentic.chat_model.anthropic_chat_model import AnthropicChatModel\n\n\n@prompt(\"Say hello\")\ndef say_hello() -&gt; str: ...\n\n\n@prompt(\n    \"Say hello\",\n    model=AnthropicChatModel(\"claude-3-5-sonnet-latest\"),\n)\ndef say_hello_anthropic() -&gt; str: ...\n\n\nsay_hello()  # Uses env vars or default settings\n\nwith OpenaiChatModel(\"gpt-4o-mini\", temperature=1):\n    say_hello()  # Uses openai with gpt-4o-mini and temperature=1 due to context manager\n    say_hello_anthropic()  # Uses Anthropic claude-3-5-sonnet-latest because explicitly configured\n</code></pre> <p>The following environment variables can be set.</p> Environment Variable Description Example MAGENTIC_BACKEND The package to use as the LLM backend anthropic / openai / litellm MAGENTIC_ANTHROPIC_MODEL Anthropic model claude-3-haiku-20240307 MAGENTIC_ANTHROPIC_API_KEY Anthropic API key to be used by magentic sk-... MAGENTIC_ANTHROPIC_BASE_URL Base URL for an Anthropic-compatible API http://localhost:8080 MAGENTIC_ANTHROPIC_MAX_TOKENS Max number of generated tokens 1024 MAGENTIC_ANTHROPIC_TEMPERATURE Temperature 0.5 MAGENTIC_LITELLM_MODEL LiteLLM model claude-2 MAGENTIC_LITELLM_API_BASE The base url to query http://localhost:11434 MAGENTIC_LITELLM_MAX_TOKENS LiteLLM max number of generated tokens 1024 MAGENTIC_LITELLM_TEMPERATURE LiteLLM temperature 0.5 MAGENTIC_MISTRAL_MODEL Mistral model mistral-large-latest MAGENTIC_MISTRAL_API_KEY Mistral API key to be used by magentic XEG... MAGENTIC_MISTRAL_BASE_URL Base URL for an Mistral-compatible API http://localhost:8080 MAGENTIC_MISTRAL_MAX_TOKENS Max number of generated tokens 1024 MAGENTIC_MISTRAL_SEED Seed for deterministic sampling 42 MAGENTIC_MISTRAL_TEMPERATURE Temperature 0.5 MAGENTIC_OPENAI_MODEL OpenAI model gpt-4 MAGENTIC_OPENAI_API_KEY OpenAI API key to be used by magentic sk-... MAGENTIC_OPENAI_API_TYPE Allowed options: \"openai\", \"azure\" azure MAGENTIC_OPENAI_BASE_URL Base URL for an OpenAI-compatible API http://localhost:8080 MAGENTIC_OPENAI_MAX_TOKENS OpenAI max number of generated tokens 1024 MAGENTIC_OPENAI_SEED Seed for deterministic sampling 42 MAGENTIC_OPENAI_TEMPERATURE OpenAI temperature 0.5"},{"location":"formatting/","title":"Formatting","text":""},{"location":"formatting/#the-format-method","title":"The <code>format</code> Method","text":"<p>Functions created using magentic decorators expose a <code>format</code> method that accepts the same parameters as the function itself but returns the completed prompt that will be sent to the model. For <code>@prompt</code> this method returns a string, and for <code>@chatprompt</code> it returns a list of <code>Message</code> objects. The <code>format</code> method can be used to test that the final prompt created by a magentic function is formatted as expected.</p> <pre><code>from magentic import prompt\n\n\n@prompt(\"Write a short poem about {topic}.\")\ndef create_poem(topic: str) -&gt; str: ...\n\n\ncreate_poem.format(\"fruit\")\n# 'Write a short poem about fruit.'\n</code></pre>"},{"location":"formatting/#classes-for-formatting","title":"Classes for Formatting","text":"<p>By default, when a list is used in a prompt template string it is formatted using its Python representation.</p> <pre><code>from magentic import prompt\nfrom magentic.formatting import BulletedList\n\n\n@prompt(\"Continue the list:\\n{items}\")\ndef get_next_items(items: list[str]) -&gt; list[str]: ...\n\n\nitems = [\"apple\", \"banana\", \"cherry\"]\nprint(get_next_items.format(items=items))\n# Continue the list:\n# ['apple', 'banana', 'cherry']\n</code></pre> <p>However, the LLM might respond better to a prompt in which the list is formatted more clearly or the items are numbered. The <code>BulletedList</code>, <code>NumberedList</code>, <code>BulletedDict</code> and <code>NumberedDict</code> classes are provided to enable this.</p> <p>For example, to modify the above prompt to contain a numbered list of the items, the <code>NumberedList</code> class can be used. This behaves exactly like a regular Python <code>list</code> except for how it appears when inserted into a formatted string. This class can also be used as the type annotation for <code>items</code> parameter to ensure that this prompt always contains a numbered list.</p> <pre><code>from magentic import prompt\nfrom magentic.formatting import NumberedList\n\n\n@prompt(\"Continue the list:\\n{items}\")\ndef get_next_items(items: NumberedList[str]) -&gt; list[str]: ...\n\n\nitems = NumberedList([\"apple\", \"banana\", \"cherry\"])\nprint(get_next_items.format(items=items))\n# Continue the list:\n# 1. apple\n# 2. banana\n# 3. cherry\n</code></pre>"},{"location":"formatting/#custom-formatting","title":"Custom Formatting","text":"<p>When objects are inserted into formatted strings in Python, the <code>__format__</code> method is called. By defining or modifying this method you can control how an object is converted to a string in the prompt. If you own the class you can modify the <code>__format__</code> method directly. Otherwise for third-party classes you will need to create a subcless.</p> <p>Here's an example of how to represent a dictionary as a bulleted list.</p> <pre><code>from typing import TypeVar\n\nfrom magentic import prompt\n\nK = TypeVar(\"K\")\nV = TypeVar(\"V\")\n\n\nclass BulletedDict(dict[K, V]):\n    def __format__(self, format_spec: str) -&gt; str:\n        # Here, you could use 'format_spec' to customize the formatting further if needed\n        return \"\\n\".join(f\"- {key}: {value}\" for key, value in self.items())\n\n\n@prompt(\"Identify the odd one out:\\n{items}\")\ndef find_odd_one_out(items: BulletedDict[str, str]) -&gt; str: ...\n\n\nitems = BulletedDict({\"sky\": \"blue\", \"grass\": \"green\", \"sun\": \"purple\"})\nprint(find_odd_one_out.format(items))\n# Identify the odd one out:\n# - sky: blue\n# - grass: green\n# - sun: purple\n</code></pre>"},{"location":"function-calling/","title":"Function Calling","text":"<p>For many use cases, it is useful to provide the LLM with tools that it can choose when and how to use. In magentic this is done by passing a list of Python functions to the <code>functions</code> argument of a magentic decorator.</p> <p>If the LLM chooses to call a function, the decorated function will return a <code>FunctionCall</code> instance. This object can be called to execute the function with the arguments that the LLM provided.</p> <pre><code>from typing import Literal\n\nfrom magentic import prompt, FunctionCall\n\n\ndef search_twitter(query: str, category: Literal[\"latest\", \"people\"]) -&gt; str:\n    \"\"\"Searches Twitter for a query.\"\"\"\n    print(f\"Searching Twitter for {query!r} in category {category!r}\")\n    return \"&lt;twitter results&gt;\"\n\n\ndef search_youtube(query: str, channel: str = \"all\") -&gt; str:\n    \"\"\"Searches YouTube for a query.\"\"\"\n    print(f\"Searching YouTube for {query!r} in channel {channel!r}\")\n    return \"&lt;youtube results&gt;\"\n\n\n@prompt(\n    \"Use the appropriate search function to answer: {question}\",\n    functions=[search_twitter, search_youtube],\n)\ndef perform_search(question: str) -&gt; FunctionCall[str]: ...\n\n\noutput = perform_search(\"What is the latest news on LLMs?\")\nprint(output)\n# &gt; FunctionCall(&lt;function search_twitter at 0x10c367d00&gt;, 'LLMs', 'latest')\noutput()\n# &gt; Searching Twitter for 'Large Language Models news' in category 'latest'\n# '&lt;twitter results&gt;'\n</code></pre>"},{"location":"function-calling/#functioncall","title":"FunctionCall","text":"<p>A <code>FunctionCall</code> combines a function with a set of arguments, ready to be called with no additional inputs required. In magentic, each time the LLM chooses to invoke a function a <code>FunctionCall</code> instance is returned. This allows the chosen function and supplied arguments to be validated or logged before the function is executed.</p> <pre><code>from magentic import FunctionCall\n\n\ndef plus(a: int, b: int) -&gt; int:\n    return a + b\n\n\nplus_1_2 = FunctionCall(plus, 1, b=2)\nprint(plus_1_2.function)\n# &gt; &lt;function plus at 0x10c39cd30&gt;\nprint(plus_1_2.arguments)\n# &gt; {'a': 1, 'b': 2}\nplus_1_2()\n# 3\n</code></pre>"},{"location":"function-calling/#prompt_chain","title":"@prompt_chain","text":"<p>In some cases, you need the model to perform multiple function calls to reach a final answer. The <code>@prompt_chain</code> decorator will execute function calls automatically, append the result to the list of messages, and query the LLM again until a final answer is reached.</p> <p>In the following example, when <code>describe_weather</code> is called the LLM first calls the <code>get_current_weather</code> function, then uses the result of this to formulate its final answer which gets returned.</p> <pre><code>from magentic import prompt_chain\n\n\ndef get_current_weather(location, unit=\"fahrenheit\"):\n    \"\"\"Get the current weather in a given location\"\"\"\n    # Pretend to query an API\n    return {\"temperature\": \"72\", \"forecast\": [\"sunny\", \"windy\"]}\n\n\n@prompt_chain(\n    \"What's the weather like in {city}?\",\n    functions=[get_current_weather],\n)\ndef describe_weather(city: str) -&gt; str: ...\n\n\ndescribe_weather(\"Boston\")\n# 'The current weather in Boston is 72\u00b0F and it is sunny and windy.'\n</code></pre> <p>LLM-powered functions created using <code>@prompt</code>, <code>@chatprompt</code> and <code>@prompt_chain</code> can be supplied as <code>functions</code> to other <code>@prompt</code>/<code>@prompt_chain</code> decorators, just like regular python functions!</p>"},{"location":"function-calling/#parallelfunctioncall","title":"ParallelFunctionCall","text":"<p>The most recent LLMs support \"parallel function calling\". This allows the model to call multiple functions at once. These functions can be executed concurrently, avoiding having to make several serial queries to the model.</p> <p>You can use <code>ParallelFunctionCall</code> (and <code>AsyncParallelFunctionCall</code>) as a return annotation to indicate that you expect the LLM to make one or more function calls. The returned <code>ParallelFunctionCall</code> is a container of <code>FunctionCall</code> instances. When called, it returns a tuple of their results.</p> <pre><code>from typing import Literal\n\nfrom magentic import prompt, ParallelFunctionCall\n\n\ndef search_twitter(query: str, category: Literal[\"latest\", \"people\"]) -&gt; str:\n    \"\"\"Searches Twitter for a query.\"\"\"\n    print(f\"Searching Twitter for {query!r} in category {category!r}\")\n    return \"&lt;twitter results&gt;\"\n\n\ndef search_youtube(query: str, channel: str = \"all\") -&gt; str:\n    \"\"\"Searches YouTube for a query.\"\"\"\n    print(f\"Searching YouTube for {query!r} in channel {channel!r}\")\n    return \"&lt;youtube results&gt;\"\n\n\n@prompt(\n    \"Use the appropriate search functions to answer: {question}\",\n    functions=[search_twitter, search_youtube],\n)\ndef perform_search(question: str) -&gt; ParallelFunctionCall[str]: ...\n\n\noutput = perform_search(\"What is the latest news on LLMs?\")\nprint(list(output))\n# &gt; [FunctionCall(&lt;function search_twitter at 0x10c39f760&gt;, 'LLMs', 'latest'),\n#    FunctionCall(&lt;function search_youtube at 0x10c39f7f0&gt;, 'LLMs')]\noutput()\n# &gt; Searching Twitter for 'LLMs' in category 'latest'\n# &gt; Searching YouTube for 'LLMs' in channel 'all'\n# ('&lt;twitter results&gt;', '&lt;youtube results&gt;')\n</code></pre>"},{"location":"function-calling/#parallelfunctioncall-with-chatprompt","title":"ParallelFunctionCall with @chatprompt","text":"<p>As with <code>FunctionCall</code> and Pydantic/Python objects, <code>ParallelFunctionCall</code> can be used with <code>@chatprompt</code> for few-shot prompting. In other words, to demonstrate to the LLM how/when it should use functions.</p> <pre><code>from magentic import (\n    chatprompt,\n    AssistantMessage,\n    FunctionCall,\n    FunctionResultMessage,\n    ParallelFunctionCall,\n    UserMessage,\n)\n\n\ndef plus(a: int, b: int) -&gt; int:\n    return a + b\n\n\ndef minus(a: int, b: int) -&gt; int:\n    return a - b\n\n\nplus_1_2 = FunctionCall(plus, 1, 2)\nminus_2_1 = FunctionCall(minus, 2, 1)\n\n\n@chatprompt(\n    UserMessage(\n        \"Sum 1 and 2. Also subtract 1 from 2.\",\n    ),\n    AssistantMessage(ParallelFunctionCall([plus_1_2, minus_2_1])),\n    FunctionResultMessage(3, plus_1_2),\n    FunctionResultMessage(1, minus_2_1),\n    UserMessage(\"Now add 4 to both results.\"),\n    functions=[plus, minus],\n)\ndef do_math() -&gt; ParallelFunctionCall[int]: ...\n\n\noutput = do_math()\nprint(list(output))\n# &gt; [FunctionCall(&lt;function plus at 0x10c3584c0&gt;, 3, 4),\n#    FunctionCall(&lt;function plus at 0x10c3584c0&gt;, 1, 4)]\noutput()\n# (7, 5)\n</code></pre>"},{"location":"function-calling/#annotated-parameters","title":"Annotated Parameters","text":"<p>Like with <code>BaseModel</code>, you can use pydantic's <code>Field</code> to provide additional information for individual function parameters, such as a description. Here's how you could document for the model that the <code>temperature</code> parameter of the <code>activate_oven</code> function is measured in Fahrenheit and should be less than 500.</p> <pre><code>from typing import Annotated, Literal\n\nfrom pydantic import Field\n\n\ndef activate_oven(\n    temperature: Annotated[int, Field(description=\"Temp in Fahrenheit\", lt=500)],\n    mode: Literal[\"broil\", \"bake\", \"roast\"],\n) -&gt; str:\n    \"\"\"Turn the oven on with the provided settings.\"\"\"\n    return f\"Preheating to {temperature} F with mode {mode}\"\n</code></pre>"},{"location":"function-calling/#configdict","title":"ConfigDict","text":"<p>Also like with <code>BaseModel</code>, pydantic's (or magentic's) <code>ConfigDict</code> can be used with functions to configure behavior. Under the hood, the function gets converted to a pydantic model, with every function parameter becoming a field on that model. See the Structured Outputs docs page for more information including the list of configuration options added by magentic.</p> <pre><code>from typing import Annotated, Literal\n\nfrom magentic import ConfigDict, with_config\nfrom pydantic import Field\n\n\n@with_config(ConfigDict(openai_strict=True))\ndef activate_oven(\n    temperature: Annotated[int, Field(description=\"Temp in Fahrenheit\", lt=500)],\n    mode: Literal[\"broil\", \"bake\", \"roast\"],\n) -&gt; str:\n    \"\"\"Turn the oven on with the provided settings.\"\"\"\n    return f\"Preheating to {temperature} F with mode {mode}\"\n</code></pre>"},{"location":"logging-and-tracing/","title":"Logging and Tracing","text":"<p>magentic is instrumented for logging and tracing using Pydantic Logfire. This also makes it compatible with OpenTelemetry.</p>"},{"location":"logging-and-tracing/#logging-to-stdout","title":"Logging to stdout","text":"<p>To print magentic spans and logs to stdout, first install the <code>logfire</code> Python package.</p> <pre><code>pip install logfire\n</code></pre> <p>Then configure this as needed. See the Pydantic Logfire Integrations docs for the full list of integrations and how to configure these.</p> <pre><code>import logfire\n\nlogfire.configure(send_to_logfire=False)\nlogfire.instrument_openai()  # optional, to trace OpenAI API calls\n# logfire.instrument_anthropic()  # optional, to trace Anthropic API calls\n</code></pre> <p>Now when running magentic code, all spans and logs will be printed.</p> <pre><code>23:02:34.197 Calling prompt-chain describe_weather\n23:02:34.221   Chat Completion with 'gpt-4o' [LLM]\n23:02:35.364   streaming response from 'gpt-4o' took 0.05s [LLM]\n23:02:35.365   Executing function call get_current_weather\n23:02:35.399   Chat Completion with 'gpt-4o' [LLM]\n23:02:35.992   streaming response from 'gpt-4o' took 0.12s [LLM]\n</code></pre>"},{"location":"logging-and-tracing/#using-pydantic-logfire","title":"Using Pydantic Logfire","text":"<p>Create a Pydantic Logfire account following their First Steps Guide.</p> <p>After authenticating locally and creating a project, configure logfire</p> <pre><code>import logfire\n\nlogfire.configure()  # note: `send_to_logfire` removed. This defaults to True\nlogfire.instrument_openai()  # optional, to trace OpenAI API calls\n# logfire.instrument_anthropic()  # optional, to trace Anthropic API calls\n</code></pre> <p>Now calls to magentic prompt-functions, prompt-chains etc. will become visible in the Logfire UI.</p> <p></p>"},{"location":"logging-and-tracing/#configuring-opentelemetry","title":"Configuring OpenTelemetry","text":"<p>To enable instrumentation for use with OpenTelemetry, use the following logfire configuration</p> <pre><code>import logfire\n\nlogfire.configure(\n    send_to_logfire=False,\n    service_name=\"magentic-test\",  # optional, can be set using OTEL_SERVICE_NAME env var\n)\nlogfire.instrument_openai()  # optional, to trace OpenAI API calls\n# logfire.instrument_anthropic()  # optional, to trace Anthropic API calls\n</code></pre> <p>Now logs and traces for magentic (and OpenAI, Anthropic, ...) will be available to any OTEL tracers.</p>"},{"location":"logging-and-tracing/#viewing-traces-locally","title":"Viewing Traces Locally","text":"<p>To view traces locally you can use Jaeger.</p> <p>First start the Jaeger all-in-one docker container</p> <pre><code>docker run --rm --name jaeger \\\n  -p 16686:16686 \\\n  -p 4317:4317 \\\n  -p 4318:4318 \\\n  jaegertracing/all-in-one:1.58\n</code></pre> <p>Then navigate to http://localhost:16686 to access the Jaeger UI. See the Jaeger Getting Started Guide for up-to-date instructions.</p> <p>Next, install the required OpenTelemetry exporter and configure OpenTelemetry to use this to send traces to Jaeger.</p> <pre><code>pip install opentelemetry-exporter-otlp\n</code></pre> <pre><code>from opentelemetry import trace\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n\ntrace.get_tracer_provider().add_span_processor(BatchSpanProcessor(OTLPSpanExporter()))\n</code></pre> <p>Now, traces for magentic code run locally will be visible in the Jaeger UI.</p> <p></p>"},{"location":"logging-and-tracing/#enabling-debug-logging","title":"Enabling Debug Logging","text":"<p>The neatest way to view the raw requests sent to LLM provider APIs is to use Logfire as described above. Another method is to enable debug logs for the LLM provider's Python package. The <code>openai</code> and <code>anthropic</code> packages use the standard library logger and expose an environment variable to set the log level. See the Logging section of the openai README or Logging section of the anthropic README for more information.</p> <p>Here's an example of what the debug log contains for a simple magentic prompt-function (formatted to improve readability).</p> <pre><code>import logging\n\nfrom magentic import prompt\n\nlogging.basicConfig(level=logging.DEBUG)\n\n\ndef plus(a: int, b: int) -&gt; int:\n    return a + b\n\n\n@prompt(\n    \"Say hello {n} times\",\n    functions=[plus],\n)\ndef say_hello(n: int) -&gt; str: ...\n\n\nsay_hello(2)\n# ...\n# &gt; DEBUG:openai._base_client:Request options: {\n#     \"method\": \"post\",\n#     \"url\": \"/chat/completions\",\n#     \"files\": None,\n#     \"json_data\": {\n#         \"messages\": [{\"role\": \"user\", \"content\": \"Say hello 2 times\"}],\n#         \"model\": \"gpt-3.5-turbo\",\n#         \"functions\": [\n#             {\n#                 \"name\": \"plus\",\n#                 \"parameters\": {\n#                     \"properties\": {\n#                         \"a\": {\"title\": \"A\", \"type\": \"integer\"},\n#                         \"b\": {\"title\": \"B\", \"type\": \"integer\"},\n#                     },\n#                     \"required\": [\"a\", \"b\"],\n#                     \"type\": \"object\",\n#                 },\n#             }\n#         ],\n#         \"max_tokens\": None,\n#         \"stream\": True,\n#         \"temperature\": None,\n#     },\n# }\n# ...\n</code></pre>"},{"location":"retrying/","title":"Retrying","text":""},{"location":"retrying/#llm-assisted-retries","title":"LLM-Assisted Retries","text":"<p>Occasionally the LLM returns an output that cannot be parsed into any of the output types or function calls that were requested. Additionally, the pydantic models you define might have extra validation that is not represented by the type annotations alone. In these cases, LLM-assisted retries can be used to automatically resubmit the output as well as the associated error message back to the LLM, giving it another opportunity with more information to meet the output schema requirements.</p> <p>To enable retries, simply set the <code>max_retries</code> parameter to a non-zero value in <code>@prompt</code> or <code>@chatprompt</code>.</p> <p>In this example</p> <ul> <li>the LLM first returns a country that is not Ireland</li> <li>then the pydantic model validation fails with error \"Country must be Ireland\"</li> <li>the original output as well as a message containing the error are resubmitted to the LLM</li> <li>the LLM correctly meets the output requirement returning \"Ireland\"</li> </ul> <pre><code>from typing import Annotated\n\nfrom magentic import prompt\nfrom pydantic import AfterValidator, BaseModel\n\n\ndef assert_is_ireland(v: str) -&gt; str:\n    if v != \"Ireland\":\n        raise ValueError(\"Country must be Ireland\")\n    return v\n\n\nclass Country(BaseModel):\n    name: Annotated[str, AfterValidator(assert_is_ireland)]\n    capital: str\n\n\n@prompt(\n    \"Return a country\",\n    max_retries=3,\n)\ndef get_country() -&gt; Country: ...\n\n\nget_country()\n# 05:13:55.607 Calling prompt-function get_country\n# 05:13:55.622   LLM-assisted retries enabled. Max 3\n# 05:13:55.627     Chat Completion with 'gpt-4o' [LLM]\n# 05:13:56.309     streaming response from 'gpt-4o' took 0.11s [LLM]\n# 05:13:56.310     Retrying Chat Completion. Attempt 1.\n# 05:13:56.322     Chat Completion with 'gpt-4o' [LLM]\n# 05:13:57.456     streaming response from 'gpt-4o' took 0.00s [LLM]\n#\n# Country(name='Ireland', capital='Dublin')\n</code></pre> <p>LLM-Assisted retries are intended to address cases where the LLM failed to generate valid output. Errors due to LLM provider rate limiting, internet connectivity issues, or other issues that cannot be solved by reprompting the LLM should be handled using other methods. For example jd/tenacity or hynek/stamina to retry a Python function.</p>"},{"location":"retrying/#retrychatmodel","title":"RetryChatModel","text":"<p>Under the hood, LLM-assisted retries are implemented using the <code>RetryChatModel</code> which wraps any other <code>ChatModel</code>, catches exceptions, and resubmits them to the LLM. To implement your own retry handling you can follow the pattern of this class. Please file a GitHub issue if you encounter exceptions that should be included in the LLM-assisted retries.</p> <p>To use the <code>RetryChatModel</code> directly rather than via the <code>max_retries</code> parameter, simply pass it as the <code>model</code> argument to the decorator. Extending the example above</p> <pre><code>from magentic import OpenaiChatModel\nfrom magentic.chat_model.retry_chat_model import RetryChatModel\n\n\n@prompt(\n    \"Return a country\",\n    model=RetryChatModel(OpenaiChatModel(\"gpt-4o-mini\"), max_retries=3),\n)\ndef get_country() -&gt; Country: ...\n\n\nget_country()\n</code></pre>"},{"location":"streaming/","title":"Streaming","text":"<p>The <code>StreamedStr</code> (and <code>AsyncStreamedStr</code>) class can be used to stream the output of the LLM. This allows you to process the text while it is being generated, rather than receiving the whole output at once.</p> <pre><code>from magentic import prompt, StreamedStr\n\n\n@prompt(\"Tell me about {country}\")\ndef describe_country(country: str) -&gt; StreamedStr: ...\n\n\n# Print the chunks while they are being received\nfor chunk in describe_country(\"Brazil\"):\n    print(chunk, end=\"\")\n# 'Brazil, officially known as the Federative Republic of Brazil, is ...'\n</code></pre> <p>Multiple <code>StreamedStr</code> can be created at the same time to stream LLM outputs concurrently. In the below example, generating the description for multiple countries takes approximately the same amount of time as for a single country.</p> <pre><code>from time import time\n\ncountries = [\"Australia\", \"Brazil\", \"Chile\"]\n\n\n# Generate the descriptions one at a time\nstart_time = time()\nfor country in countries:\n    # Converting `StreamedStr` to `str` blocks until the LLM output is fully generated\n    description = str(describe_country(country))\n    print(f\"{time() - start_time:.2f}s : {country} - {len(description)} chars\")\n\n# 22.72s : Australia - 2130 chars\n# 41.63s : Brazil - 1884 chars\n# 74.31s : Chile - 2968 chars\n\n\n# Generate the descriptions concurrently by creating the StreamedStrs at the same time\nstart_time = time()\nstreamed_strs = [describe_country(country) for country in countries]\nfor country, streamed_str in zip(countries, streamed_strs):\n    description = str(streamed_str)\n    print(f\"{time() - start_time:.2f}s : {country} - {len(description)} chars\")\n\n# 22.79s : Australia - 2147 chars\n# 23.64s : Brazil - 2202 chars\n# 24.67s : Chile - 2186 chars\n</code></pre>"},{"location":"streaming/#object-streaming","title":"Object Streaming","text":"<p>Structured outputs can also be streamed from the LLM by using the return type annotation <code>Iterable</code> (or <code>AsyncIterable</code>). This allows each item to be processed while the next one is being generated.</p> <pre><code>from collections.abc import Iterable\nfrom time import time\n\nfrom magentic import prompt\nfrom pydantic import BaseModel\n\n\nclass Superhero(BaseModel):\n    name: str\n    age: int\n    power: str\n    enemies: list[str]\n\n\n@prompt(\"Create a Superhero team named {name}.\")\ndef create_superhero_team(name: str) -&gt; Iterable[Superhero]: ...\n\n\nstart_time = time()\nfor hero in create_superhero_team(\"The Food Dudes\"):\n    print(f\"{time() - start_time:.2f}s : {hero}\")\n\n# 2.23s : name='Pizza Man' age=30 power='Can shoot pizza slices from his hands' enemies=['The Hungry Horde', 'The Junk Food Gang']\n# 4.03s : name='Captain Carrot' age=35 power='Super strength and agility from eating carrots' enemies=['The Sugar Squad', 'The Greasy Gang']\n# 6.05s : name='Ice Cream Girl' age=25 power='Can create ice cream out of thin air' enemies=['The Hot Sauce Squad', 'The Healthy Eaters']\n</code></pre>"},{"location":"streaming/#streamedresponse","title":"StreamedResponse","text":"<p>Some LLMs have the ability to generate text output and make tool calls in the same response. This allows them to perform chain-of-thought reasoning or provide additional context to the user. In magentic, the <code>StreamedResponse</code> (or <code>AsyncStreamedResponse</code>) class can be used to request this type of output. This object is an iterable of <code>StreamedStr</code> (or <code>AsyncStreamedStr</code>) and <code>FunctionCall</code> instances.</p> <p>Consuming StreamedStr</p> <p>The StreamedStr object caches its chunks internally, so it does not have to be consumed immediately. This means you can iterate over the chunks as they are received, and/or use the StreamedStr object as a whole after the LLM has finished generating the output.</p> <p>In the example below, we request that the LLM generates a greeting and then calls a function to get the weather for two cities. The <code>StreamedResponse</code> object is then iterated over to print the output, and the <code>StreamedStr</code> and <code>FunctionCall</code> items are processed separately.</p> <pre><code>from magentic import prompt, FunctionCall, StreamedResponse, StreamedStr\n\n\ndef get_weather(city: str) -&gt; str:\n    return f\"The weather in {city} is 20\u00b0C.\"\n\n\n@prompt(\n    \"Say hello, then get the weather for: {cities}\",\n    functions=[get_weather],\n)\ndef describe_weather(cities: list[str]) -&gt; StreamedResponse: ...\n\n\nresponse = describe_weather([\"Cape Town\", \"San Francisco\"])\nfor item in response:\n    if isinstance(item, StreamedStr):\n        for chunk in item:\n            # print the chunks as they are received\n            print(chunk, sep=\"\", end=\"\")\n        print()\n    if isinstance(item, FunctionCall):\n        # print the function call, then call it and print the result\n        print(item)\n        print(item())\n\n# Hello! I'll get the weather for Cape Town and San Francisco for you.\n# FunctionCall(&lt;function get_weather at 0x1109825c0&gt;, 'Cape Town')\n# The weather in Cape Town is 20\u00b0C.\n# FunctionCall(&lt;function get_weather at 0x1109825c0&gt;, 'San Francisco')\n# The weather in San Francisco is 20\u00b0C.\n</code></pre>"},{"location":"structured-outputs/","title":"Structured Outputs","text":""},{"location":"structured-outputs/#pydantic-models","title":"Pydantic Models","text":"<p>The <code>@prompt</code> decorator will respect the return type annotation of the decorated function. This can be any type supported by pydantic including a <code>pydantic</code> model. See the Pydantic docs for more information about models.</p> <pre><code>from magentic import prompt\nfrom pydantic import BaseModel\n\n\nclass Superhero(BaseModel):\n    name: str\n    age: int\n    power: str\n    enemies: list[str]\n\n\n@prompt(\"Create a Superhero named {name}.\")\ndef create_superhero(name: str) -&gt; Superhero: ...\n\n\ncreate_superhero(\"Garden Man\")\n# Superhero(name='Garden Man', age=30, power='Control over plants', enemies=['Pollution Man', 'Concrete Woman'])\n</code></pre>"},{"location":"structured-outputs/#using-field","title":"Using <code>Field</code>","text":"<p>With pydantic's <code>BaseModel</code>, you can use <code>Field</code> to provide additional information for individual attributes, such as a description.</p> <pre><code>from magentic import prompt\nfrom pydantic import BaseModel\n\n\nclass Superhero(BaseModel):\n    name: str\n    age: int = Field(\n        description=\"The age of the hero, which could be much older than humans.\"\n    )\n    power: str = Field(examples=[\"Runs really fast\"])\n    enemies: list[str]\n\n\n@prompt(\"Create a Superhero named {name}.\")\ndef create_superhero(name: str) -&gt; Superhero: ...\n</code></pre>"},{"location":"structured-outputs/#configdict","title":"ConfigDict","text":"<p>Pydantic also supports configuring the <code>BaseModel</code> by setting the <code>model_config</code> attribute. Magentic extends pydantic's <code>ConfigDict</code> class to add the following additional configuration options</p> <ul> <li><code>openai_strict: bool</code> Indicates whether to use OpenAI's Structured Outputs feature.</li> </ul> <p>See the pydantic Configuration docs for the inherited configuration options.</p> <pre><code>from magentic import prompt, ConfigDict\nfrom pydantic import BaseModel\n\n\nclass Superhero(BaseModel):\n    model_config = ConfigDict(openai_strict=True)\n\n    name: str\n    age: int\n    power: str\n    enemies: list[str]\n\n\n@prompt(\"Create a Superhero named {name}.\")\ndef create_superhero(name: str) -&gt; Superhero: ...\n\n\ncreate_superhero(\"Garden Man\")\n</code></pre>"},{"location":"structured-outputs/#json-schema","title":"JSON Schema","text":"<p>OpenAI Structured Outputs</p> <p>Setting <code>openai_strict=True</code> results in a different JSON schema than that from <code>.model_json_schema()</code> being sent to the LLM. Use <code>openai.pydantic_function_tool(Superhero)</code> to generate the JSON schema in this case.</p> <p>You can generate the JSON schema for the pydantic model using the <code>.model_json_schema()</code> method. This is what is sent to the LLM.</p> <p>Running <code>Superhero.model_json_schema()</code> for the above definition reveals the following JSON schema</p> <pre><code>{\n    \"properties\": {\n        \"name\": {\"title\": \"Name\", \"type\": \"string\"},\n        \"age\": {\n            \"description\": \"The age of the hero, which could be much older than humans.\",\n            \"title\": \"Age\",\n            \"type\": \"integer\",\n        },\n        \"power\": {\n            \"examples\": [\"Runs really fast\"],\n            \"title\": \"Power\",\n            \"type\": \"string\",\n        },\n        \"enemies\": {\"items\": {\"type\": \"string\"}, \"title\": \"Enemies\", \"type\": \"array\"},\n    },\n    \"required\": [\"name\", \"age\", \"power\", \"enemies\"],\n    \"title\": \"Superhero\",\n    \"type\": \"object\",\n}\n</code></pre> <p>If a <code>StructuredOutputError</code> is raised often, this indicates that the LLM is failing to match the schema. The traceback for these errors includes the underlying pydantic <code>ValidationError</code> which shows in what way the received response was invalid. To combat these errors there are several options</p> <ul> <li>Add descriptions or examples for individual fields to demonstrate valid values.</li> <li>Simplify the output schema, including using more flexible types (e.g. <code>str</code> instead of <code>datetime</code>) or allowing fields to be nullable with <code>| None</code>.</li> <li>Switch to a \"more intelligent\" LLM. See Configuration for how to do this.</li> </ul>"},{"location":"structured-outputs/#python-types","title":"Python Types","text":"<p>Regular Python types can also be used as the function return type.</p> <pre><code>from magentic import prompt\nfrom pydantic import BaseModel, Field\n\n\nclass Superhero(BaseModel):\n    name: str\n    age: int\n    power: str\n    enemies: list[str]\n\n\ngarden_man = Superhero(\n    name=\"Garden Man\",\n    age=30,\n    power=\"Control over plants\",\n    enemies=[\"Pollution Man\", \"Concrete Woman\"],\n)\n\n\n@prompt(\"Return True if {hero.name} will be defeated by enemies {hero.enemies}\")\ndef will_be_defeated(hero: Superhero) -&gt; bool: ...\n\n\nhero_defeated = will_be_defeated(garden_man)\nprint(hero_defeated)\n# &gt; True\n</code></pre>"},{"location":"structured-outputs/#chain-of-thought-prompting","title":"Chain-of-Thought Prompting","text":"<p>StreamedResponse</p> <p>It is now recommended to use <code>StreamedResponse</code> for chain-of-thought prompting, as this uses the LLM provider's native chain-of-thought capabilities. See StreamedResponse for more information.</p> <p>Using a simple Python type as the return annotation might result in poor results as the LLM has no time to arrange its thoughts before answering. To allow the LLM to work through this \"chain of thought\" you can instead return a pydantic model with initial fields for explaining the final response.</p> <pre><code>from magentic import prompt\nfrom pydantic import BaseModel, Field\n\n\nclass ExplainedDefeated(BaseModel):\n    explanation: str = Field(\n        description=\"Describe the battle between the hero and their enemy.\"\n    )\n    defeated: bool = Field(description=\"True if the hero was defeated.\")\n\n\nclass Superhero(BaseModel):\n    name: str\n    age: int\n    power: str\n    enemies: list[str]\n\n\n@prompt(\"Return True if {hero.name} will be defeated by enemies {hero.enemies}\")\ndef will_be_defeated(hero: Superhero) -&gt; ExplainedDefeated: ...\n\n\ngarden_man = Superhero(\n    name=\"Garden Man\",\n    age=30,\n    power=\"Control over plants\",\n    enemies=[\"Pollution Man\", \"Concrete Woman\"],\n)\n\nhero_defeated = will_be_defeated(garden_man)\nprint(hero_defeated.defeated)\n# &gt; True\nprint(hero_defeated.explanation)\n# &gt; 'Garden Man is an environmental hero who fights against Pollution Man ...'\n</code></pre>"},{"location":"structured-outputs/#explained","title":"Explained","text":"<p>Using chain-of-thought is a common approach to improve the output of the model, so a generic <code>Explained</code> model might be generally useful. The <code>description</code> or <code>example</code> parameters of <code>Field</code> can be used to demonstrate the desired style and detail of the explanations.</p> <pre><code>from typing import Generic, TypeVar\n\nfrom magentic import prompt\nfrom pydantic import BaseModel, Field\n\n\nT = TypeVar(\"T\")\n\n\nclass Explained(BaseModel, Generic[T]):\n    explanation: str = Field(description=\"Explanation of how the value was determined.\")\n    value: T\n\n\n@prompt(\"Return True if {hero.name} will be defeated by enemies {hero.enemies}\")\ndef will_be_defeated(hero: Superhero) -&gt; Explained[bool]: ...\n</code></pre>"},{"location":"type-checking/","title":"Type Checking","text":"<p>Many type checkers will raise warnings or errors for functions with the <code>@prompt</code> decorator due to the function having no body or return value. There are several ways to deal with these.</p> <ol> <li>Disable the check globally for the type checker. For example in mypy by disabling error code <code>empty-body</code>.    <pre><code># pyproject.toml\n[tool.mypy]\ndisable_error_code = [\"empty-body\"]\n</code></pre></li> <li>Make the function body <code>...</code> (this does not satisfy mypy) or <code>raise</code>.    <pre><code>@prompt(\"Choose a color\")\ndef random_color() -&gt; str: ...\n</code></pre></li> <li>Use comment <code># type: ignore[empty-body]</code> on each function. In this case you can add a docstring instead of <code>...</code>.    <pre><code>@prompt(\"Choose a color\")\ndef random_color() -&gt; str:  # type: ignore[empty-body]\n    \"\"\"Returns a random color.\"\"\"\n</code></pre></li> </ol>"},{"location":"vision/","title":"Vision","text":"<p>Image inputs can be provided to LLMs in magentic by using <code>ImageBytes</code> or <code>ImageUrl</code> within the <code>UserMessage</code> message type. The LLM used must support vision, for example <code>gpt-4o</code> (the default <code>ChatModel</code>). The model can be set by passing the <code>model</code> parameter to <code>@chatprompt</code>, or through the other methods of configuration.</p> <p>Anthropic Image URLs</p> <p>Anthropic models currently do not support supplying an image as a url, just bytes.</p> <p>For more information visit the OpenAI Vision API documentation or the Anthropic Vision API documentation.</p> <p>Document inputs can also be provided using the <code>DocumentBytes</code> object. This is currently only supported by some Anthropic models. See the Anthropic PDF Support documentation for more information.</p> <p>UserImageMessage Deprecation</p> <p>Previously the <code>UserImageMessage</code> was used for vision capabilities. This is now deprecated and will be removed in a future version of Magentic. It is recommended to use <code>ImageBytes</code> or <code>ImageUrl</code> within the <code>UserMessage</code> message type instead to ensure compatibility with future updates.</p>"},{"location":"vision/#imageurl","title":"ImageUrl","text":"<p>As shown in Chat Prompting, <code>@chatprompt</code> can be used to supply a group of messages as a prompt to the LLM. <code>UserMessage</code> accepts a sequence of content blocks as input, which can be <code>str</code>, <code>ImageBytes</code>, <code>ImageUrl</code>, or other content types. <code>ImageUrl</code> is used to provide an image url to the LLM.</p> <pre><code>from pydantic import BaseModel, Field\n\nfrom magentic import chatprompt, ImageUrl, UserMessage\n\n\nIMAGE_URL_WOODEN_BOARDWALK = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n\n\nclass ImageDetails(BaseModel):\n    description: str = Field(description=\"A brief description of the image.\")\n    name: str = Field(description=\"A short name.\")\n\n\n@chatprompt(\n    UserMessage(\n        [\n            \"Describe the following image in one sentence.\",\n            ImageUrl(IMAGE_URL_WOODEN_BOARDWALK),\n        ]\n    ),\n)\ndef describe_image() -&gt; ImageDetails: ...\n\n\nimage_details = describe_image()\nprint(image_details.name)\n# 'Wooden Boardwalk in Green Wetland'\nprint(image_details.description)\n# 'A serene wooden boardwalk meanders through a lush green wetland under a blue sky dotted with clouds.'\n</code></pre> <p>For more info on the <code>@chatprompt</code> decorator, see Chat Prompting.</p>"},{"location":"vision/#placeholder","title":"Placeholder","text":"<p>In the previous example, the image url was tied to the function. To provide the image as a function parameter, use <code>Placeholder</code>. This substitutes a function argument into the message when the function is called. The placeholder will also automatically coerce the argument to the correct type if possible, for example <code>str</code> to <code>ImageUrl</code>.</p> <pre><code>from magentic import chatprompt, ImageUrl, Placeholder, UserMessage\n\n\nIMAGE_URL_WOODEN_BOARDWALK = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n\n\n@chatprompt(\n    UserMessage(\n        [\n            \"Describe the following image in one sentence.\",\n            Placeholder(ImageUrl, \"image_url\"),\n        ]\n    ),\n)\ndef describe_image(image_url: str) -&gt; str: ...\n\n\ndescribe_image(IMAGE_URL_WOODEN_BOARDWALK)\n# 'A wooden boardwalk meanders through lush green wetlands under a partly cloudy blue sky.'\n</code></pre>"},{"location":"vision/#imagebytes","title":"ImageBytes","text":"<p><code>UserMessage</code> can also accept <code>ImageBytes</code> as a content block. Like <code>ImageUrl</code>, this can be passed directly or via <code>Placeholder</code>.</p> <pre><code>import requests\n\nfrom magentic import chatprompt, ImageBytes, Placeholder, UserMessage\n\n\nIMAGE_URL_WOODEN_BOARDWALK = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n\n\ndef url_to_bytes(url: str) -&gt; bytes:\n    \"\"\"Get the content of a URL as bytes.\"\"\"\n\n    # A custom user-agent is necessary to comply with Wikimedia user-agent policy\n    # https://meta.wikimedia.org/wiki/User-Agent_policy\n    headers = {\"User-Agent\": \"MagenticExampleBot (https://magentic.dev/)\"}\n    return requests.get(url, headers=headers, timeout=10).content\n\n\n@chatprompt(\n    UserMessage(\n        [\n            \"Describe the following image in one sentence.\",\n            Placeholder(ImageBytes, \"image_bytes\"),\n        ]\n    ),\n)\ndef describe_image(image_bytes: bytes) -&gt; str: ...\n\n\nimage_bytes = url_to_bytes(IMAGE_URL_WOODEN_BOARDWALK)\ndescribe_image(image_bytes)\n# 'The image shows a wooden boardwalk extending through a lush green wetland with a backdrop of blue skies and scattered clouds.'\n</code></pre>"},{"location":"vision/#documentbytes","title":"DocumentBytes","text":"<p><code>DocumentBytes</code> is used to provide a document as bytes to the LLM. This is currently only supported by some Anthropic models.</p> <pre><code>from pathlib import Path\n\nfrom magentic import chatprompt, DocumentBytes, Placeholder, UserMessage\nfrom magentic.chat_model.anthropic_chat_model import AnthropicChatModel\n\n\n@chatprompt(\n    UserMessage(\n        [\n            \"Repeat the contents of this document.\",\n            Placeholder(DocumentBytes, \"document_bytes\"),\n        ]\n    ),\n    model=AnthropicChatModel(\"claude-3-5-sonnet-20241022\"),\n)\ndef read_document(document_bytes: bytes) -&gt; str: ...\n\n\ndocument_bytes = Path(\"...\").read_bytes()\nread_document(document_bytes)\n# 'This is a test PDF.'\n</code></pre>"},{"location":"examples/chain_of_verification/","title":"Chain of Verification (CoVe)","text":"In\u00a0[1]: Copied! <pre># Define the prompts\n\nimport asyncio\n\nfrom magentic import prompt\n\n\n@prompt(\"{query}\")\nasync def answer_query(query: str) -&gt; str: ...\n\n\n@prompt(\n    \"\"\"\\\nQuery: {query}\nResponse: {response}\n\nProvide specific questions to verify the facts in the above response as related to the query.\n\"\"\"\n)\nasync def generate_verification_questions(query: str, response: str) -&gt; list[str]: ...\n\n\n@prompt(\n    \"\"\"\\\n{context}\n\nGiven the above context, what is the answer to the following question?\n\n{query}\"\"\"\n)\nasync def answer_query_with_context(query: str, context: str) -&gt; str: ...\n</pre> # Define the prompts  import asyncio  from magentic import prompt   @prompt(\"{query}\") async def answer_query(query: str) -&gt; str: ...   @prompt(     \"\"\"\\ Query: {query} Response: {response}  Provide specific questions to verify the facts in the above response as related to the query. \"\"\" ) async def generate_verification_questions(query: str, response: str) -&gt; list[str]: ...   @prompt(     \"\"\"\\ {context}  Given the above context, what is the answer to the following question?  {query}\"\"\" ) async def answer_query_with_context(query: str, context: str) -&gt; str: ... In\u00a0[2]: Copied! <pre># 1. Generate Baseline Response\n# Given a query, generate the response using the LLM.\n\nquery = \"Name some politicians who were born in NY, New York\"\n\nbaseline_response = await answer_query(query)\nprint(baseline_response)\n</pre> # 1. Generate Baseline Response # Given a query, generate the response using the LLM.  query = \"Name some politicians who were born in NY, New York\"  baseline_response = await answer_query(query) print(baseline_response) <pre>Here are some politicians who were born in New York, New York:\n\n1. Franklin D. Roosevelt - 32nd President of the United States.\n2. Theodore Roosevelt - 26th President of the United States.\n3. Donald Trump - 45th President of the United States.\n4. Hillary Clinton - Former Secretary of State and Democratic nominee for President in 2016.\n5. Michael Bloomberg - Former Mayor of New York City and businessman.\n6. Rudy Giuliani - Former Mayor of New York City and attorney.\n7. Chuck Schumer - U.S. Senator from New York and current Senate Majority Leader.\n8. Kirsten Gillibrand - U.S. Senator from New York.\n9. Mario Cuomo - Former Governor of New York.\n10. Andrew Cuomo - Current Governor of New York.\n\nPlease note that this is not an exhaustive list, and there are many more politicians who were born in New York, New York.\n</pre> In\u00a0[3]: Copied! <pre># 2. Plan Verifications\n# Given both query and baseline response, generate a list of verification questions\n# that could help to self-analyze if there are any mistakes in the original response.\n\nverification_questions = await generate_verification_questions(query, baseline_response)\n\nfor q in verification_questions:\n    print(q)\n</pre> # 2. Plan Verifications # Given both query and baseline response, generate a list of verification questions # that could help to self-analyze if there are any mistakes in the original response.  verification_questions = await generate_verification_questions(query, baseline_response)  for q in verification_questions:     print(q) <pre>Was Franklin D. Roosevelt born in New York, New York?\nWas Theodore Roosevelt born in New York, New York?\nWas Donald Trump born in New York, New York?\nWas Hillary Clinton born in New York, New York?\nWas Michael Bloomberg born in New York, New York?\nWas Rudy Giuliani born in New York, New York?\nWas Chuck Schumer born in New York, New York?\nWas Kirsten Gillibrand born in New York, New York?\nWas Mario Cuomo born in New York, New York?\nIs Andrew Cuomo the current Governor of New York?\n</pre> In\u00a0[4]: Copied! <pre># 3. Execute Verifications\n# Answer each verification question in turn, and hence check the answer against the\n# original response to check for inconsistencies or mistakes.\n\nverification_answers = await asyncio.gather(\n    *(answer_query(question) for question in verification_questions)\n)\n\nfor ans in verification_answers:\n    print(ans)\n</pre> # 3. Execute Verifications # Answer each verification question in turn, and hence check the answer against the # original response to check for inconsistencies or mistakes.  verification_answers = await asyncio.gather(     *(answer_query(question) for question in verification_questions) )  for ans in verification_answers:     print(ans) <pre>Yes, Franklin D. Roosevelt was born on January 30, 1882, in Hyde Park, New York, which is located in Dutchess County.\nYes, Theodore Roosevelt was born in New York City, New York on October 27, 1858. Specifically, he was born in a house located at 28 East 20th Street in Manhattan.\nYes, Donald Trump was indeed born in New York, New York on June 14, 1946.\nNo, Hillary Clinton was not born in New York, New York. She was born on October 26, 1947, in Chicago, Illinois.\nNo, Michael Bloomberg was born in Boston, Massachusetts on February 14, 1942.\nYes, Rudy Giuliani was born in New York, New York. He was born on May 28, 1944, in the Brooklyn borough of New York City.\nYes, Chuck Schumer was born in Brooklyn, New York on November 23, 1950.\nNo, Kirsten Gillibrand was born in Albany, New York on December 9, 1966.\nYes, Mario Cuomo was born in New York City, New York, United States. He was born on June 15, 1932, in the borough of Queens, specifically in the neighborhood of South Jamaica.\nAs of September 2021, Andrew Cuomo is the current Governor of New York. However, please note that political positions can change, and it is always recommended to verify the information with up-to-date sources.\n</pre> In\u00a0[5]: Copied! <pre># 4. Generate Final Verified Response\n# Given the discovered inconsistencies (if any), generate a revised response\n# incorporating the verification results.\n\ncontext = \"\\n\".join(verification_answers)\nverified_response = await answer_query_with_context(query, context)\nprint(verified_response)\n</pre> # 4. Generate Final Verified Response # Given the discovered inconsistencies (if any), generate a revised response # incorporating the verification results.  context = \"\\n\".join(verification_answers) verified_response = await answer_query_with_context(query, context) print(verified_response) <pre>Some politicians who were born in New York, New York include Franklin D. Roosevelt, Theodore Roosevelt, Donald Trump, Rudy Giuliani, and Mario Cuomo.\n</pre>"},{"location":"examples/chain_of_verification/#chain-of-verification-cove","title":"Chain of Verification (CoVe)\u00b6","text":"<p>This notebook is a basic implementation of the paper Chain-of-Verification Reduces Hallucination In Large Language Models (2023) (arXiv: [2309.11495]).</p>"},{"location":"examples/rag_github/","title":"Retrieval-Augmented Generation with GitHub","text":"In\u00a0[\u00a0]: Copied! <pre># Install dependencies (skip this cell if already installed)\n! pip install magentic\n! pip install ghapi\n</pre> # Install dependencies (skip this cell if already installed) ! pip install magentic ! pip install ghapi In\u00a0[2]: Copied! <pre># Configure magentic to use the `gpt-3.5-turbo` model for this notebook\n%env MAGENTIC_OPENAI_MODEL=gpt-3.5-turbo\n</pre> # Configure magentic to use the `gpt-3.5-turbo` model for this notebook %env MAGENTIC_OPENAI_MODEL=gpt-3.5-turbo <pre>env: MAGENTIC_OPENAI_MODEL=gpt-3.5-turbo\n</pre> <p>Let's start by creating a prompt-function to generate some text recommending GitHub repos for a topic.</p> In\u00a0[3]: Copied! <pre># Create a prompt-function to describe the latest GitHub repos\n\nfrom IPython.display import Markdown, display\n\nfrom magentic import prompt\n\n\n@prompt(\n    \"\"\"What are the latest github repos I should use related to {topic}?\n    Recommend three in particular that I should check out and why.\n    Provide a link to each, and a note on whether they are actively maintained.\n    \"\"\"\n)\ndef recommmend_github_repos(topic: str) -&gt; str: ...\n\n\noutput = recommmend_github_repos(\"LLMs\")\ndisplay(Markdown(output))\n</pre> # Create a prompt-function to describe the latest GitHub repos  from IPython.display import Markdown, display  from magentic import prompt   @prompt(     \"\"\"What are the latest github repos I should use related to {topic}?     Recommend three in particular that I should check out and why.     Provide a link to each, and a note on whether they are actively maintained.     \"\"\" ) def recommmend_github_repos(topic: str) -&gt; str: ...   output = recommmend_github_repos(\"LLMs\") display(Markdown(output)) <ol> <li>Hugging Face Transformers: This repository contains a library for Natural Language Processing (NLP) tasks using the latest Transformer models, including LLMs. It is actively maintained by Hugging Face, a popular NLP research group, and has a large community contributing to it.</li> </ol> <p>Link: https://github.com/huggingface/transformers</p> <ol> <li>OpenAI GPT-3: This repository contains the code for OpenAI's GPT-3 model, one of the most advanced LLMs available. While the repository may not be frequently updated due to proprietary restrictions, it provides valuable insights into how state-of-the-art LLMs are implemented.</li> </ol> <p>Link: https://github.com/openai/gpt-3</p> <ol> <li>AllenNLP: AllenNLP is a deep learning library for NLP research that provides easy-to-use tools for building and experimenting with LLMs. The repository is actively maintained by the Allen Institute for AI and offers a wide range of pre-trained models, including BERT and GPT-2.</li> </ol> <p>Link: https://github.com/allenai/allennlp</p> <p>Please note that the availability and maintenance status of these repositories may change over time, so it's a good idea to check for the latest updates before diving in.</p> <p>The LLM has no knowledge of GitHub repos created after its knowledge cutoff date! Also, it occasionally hallucinates some of its answers. To solve these issues we need to provide it with up-to-date information in the prompt, which it can use to generate an informed answer.</p> <p>First we'll create a function for searching for GitHub repos.</p> In\u00a0[4]: Copied! <pre># Create a function to search for GitHub repos\n\nfrom ghapi.all import GhApi\nfrom pydantic import BaseModel\n\ngithub = GhApi(authenticate=False)\n\n\nclass GithubRepo(BaseModel):\n    full_name: str\n    description: str\n    html_url: str\n    stargazers_count: int\n    pushed_at: str\n\n\ndef search_github_repos(query: str, num_results: int = 10):\n    results = github.search.repos(query, per_page=num_results)\n    return [GithubRepo.model_validate(item) for item in results[\"items\"]]\n</pre> # Create a function to search for GitHub repos  from ghapi.all import GhApi from pydantic import BaseModel  github = GhApi(authenticate=False)   class GithubRepo(BaseModel):     full_name: str     description: str     html_url: str     stargazers_count: int     pushed_at: str   def search_github_repos(query: str, num_results: int = 10):     results = github.search.repos(query, per_page=num_results)     return [GithubRepo.model_validate(item) for item in results[\"items\"]] In\u00a0[5]: Copied! <pre># Test that github search works\n\nfor item in search_github_repos(\"openai\", num_results=3):\n    print(item.model_dump_json(indent=2))\n</pre> # Test that github search works  for item in search_github_repos(\"openai\", num_results=3):     print(item.model_dump_json(indent=2)) <pre>{\n  \"full_name\": \"openai/openai-cookbook\",\n  \"description\": \"Examples and guides for using the OpenAI API\",\n  \"html_url\": \"https://github.com/openai/openai-cookbook\",\n  \"stargazers_count\": 55805,\n  \"pushed_at\": \"2024-04-19T19:05:02Z\"\n}\n{\n  \"full_name\": \"betalgo/openai\",\n  \"description\": \"OpenAI .NET sdk - Azure OpenAI, ChatGPT, Whisper,  and DALL-E \",\n  \"html_url\": \"https://github.com/betalgo/openai\",\n  \"stargazers_count\": 2721,\n  \"pushed_at\": \"2024-04-20T22:50:28Z\"\n}\n{\n  \"full_name\": \"openai/openai-python\",\n  \"description\": \"The official Python library for the OpenAI API\",\n  \"html_url\": \"https://github.com/openai/openai-python\",\n  \"stargazers_count\": 19786,\n  \"pushed_at\": \"2024-04-21T01:04:42Z\"\n}\n</pre> <p>Now, we can provide the results of the search as context to the LLM to create an improved <code>recommmend_github_repos</code> function.</p> In\u00a0[6]: Copied! <pre># Combine the search with a prompt-function to describe the latest GitHub repos\n\nfrom magentic import prompt\n\n\n@prompt(\n    \"\"\"What are the latest github repos I should use related to {topic}?\n    Recommend three in particular that I should check out and why.\n    Provide a link to each, and a note on whether they are actively maintained.\n\n    Here are the latest search results for this topic on GitHub:\n    {search_results}\n    \"\"\",\n)\ndef recommmend_github_repos_using_search_results(\n    topic: str, search_results: list[GithubRepo]\n) -&gt; str: ...\n\n\ndef recommmend_github_repos(topic: str) -&gt; str:\n    search_results = search_github_repos(topic, num_results=10)\n    return recommmend_github_repos_using_search_results(topic, search_results)\n\n\noutput = recommmend_github_repos(\"LLMs\")\ndisplay(Markdown(output))\n</pre> # Combine the search with a prompt-function to describe the latest GitHub repos  from magentic import prompt   @prompt(     \"\"\"What are the latest github repos I should use related to {topic}?     Recommend three in particular that I should check out and why.     Provide a link to each, and a note on whether they are actively maintained.      Here are the latest search results for this topic on GitHub:     {search_results}     \"\"\", ) def recommmend_github_repos_using_search_results(     topic: str, search_results: list[GithubRepo] ) -&gt; str: ...   def recommmend_github_repos(topic: str) -&gt; str:     search_results = search_github_repos(topic, num_results=10)     return recommmend_github_repos_using_search_results(topic, search_results)   output = recommmend_github_repos(\"LLMs\") display(Markdown(output)) <p>Based on the latest search results, here are three GitHub repos related to Large Language Models (LLMs) that you should check out:</p> <ol> <li><p>gpt4all:</p> <ul> <li>Description: gpt4all: run open-source LLMs anywhere</li> <li>Stargazers Count: 63,790</li> <li>Last Pushed: 2024-04-19</li> <li>Active Maintenance: Yes</li> </ul> </li> <li><p>LLaMA-Factory:</p> <ul> <li>Description: Unify Efficient Fine-Tuning of 100+ LLMs</li> <li>Stargazers Count: 17,047</li> <li>Last Pushed: 2024-04-21</li> <li>Active Maintenance: Yes</li> </ul> </li> <li><p>LLMsPracticalGuide:</p> <ul> <li>Description: A curated list of practical guide resources of LLMs (LLMs Tree, Examples, Papers)</li> <li>Stargazers Count: 8,484</li> <li>Last Pushed: 2024-01-10</li> <li>Active Maintenance: It seems less actively maintained compared to the other two repos, but still worth checking out.</li> </ul> </li> </ol> <p>These repos cover a range of topics related to LLMs and can provide valuable resources and tools for your projects.</p> <p>Now the answer contains up-to-date and correct information!</p>"},{"location":"examples/rag_github/#retrieval-augmented-generation-with-github","title":"Retrieval-Augmented Generation with GitHub\u00b6","text":"<p>This notebook demonstrates how to perform Retrieval-Augmented Generation (RAG) with magentic using the GitHub API. Essentially, RAG provides context to the LLM which it can use when generating its response. This approach allows us to insert new or private information that was not present in the model's training data.</p>"},{"location":"examples/registering_custom_type/","title":"Registering a Custom Type","text":"In\u00a0[1]: Copied! <pre># Create FunctionSchema for pd.DataFrame\n\nimport json\nfrom collections.abc import Iterable\nfrom typing import Any\n\nimport pandas as pd\n\nfrom magentic.chat_model.function_schema import FunctionSchema, register_function_schema\n\n\n@register_function_schema(pd.DataFrame)\nclass DataFrameFunctionSchema(FunctionSchema[pd.DataFrame]):\n    @property\n    def name(self) -&gt; str:\n        \"\"\"The name of the function.\n\n        Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.\n        \"\"\"\n        return \"dataframe\"\n\n    @property\n    def description(self) -&gt; str | None:\n        return \"A DataFrame object.\"\n\n    @property\n    def parameters(self) -&gt; dict[str, Any]:\n        \"\"\"The parameters the functions accepts as a JSON Schema object.\"\"\"\n        return {\n            \"properties\": {\n                \"columns\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n                \"data\": {\n                    \"type\": \"array\",\n                    \"items\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n                },\n            },\n            \"required\": [\"index\", \"columns\", \"data\"],\n            \"type\": \"object\",\n        }\n\n    def parse_args(self, chunks: Iterable[str]) -&gt; pd.DataFrame:\n        \"\"\"Parse an iterable of string chunks into the function arguments.\"\"\"\n        args = json.loads(\"\".join(chunks))\n        return pd.DataFrame(**args)\n\n    def serialize_args(self, value: pd.DataFrame) -&gt; dict:\n        \"\"\"Serialize an object into a JSON string of function arguments.\"\"\"\n        return {\n            \"columns\": value.columns.tolist(),\n            \"data\": value.to_numpy().tolist(),\n        }\n</pre> # Create FunctionSchema for pd.DataFrame  import json from collections.abc import Iterable from typing import Any  import pandas as pd  from magentic.chat_model.function_schema import FunctionSchema, register_function_schema   @register_function_schema(pd.DataFrame) class DataFrameFunctionSchema(FunctionSchema[pd.DataFrame]):     @property     def name(self) -&gt; str:         \"\"\"The name of the function.          Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.         \"\"\"         return \"dataframe\"      @property     def description(self) -&gt; str | None:         return \"A DataFrame object.\"      @property     def parameters(self) -&gt; dict[str, Any]:         \"\"\"The parameters the functions accepts as a JSON Schema object.\"\"\"         return {             \"properties\": {                 \"columns\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},                 \"data\": {                     \"type\": \"array\",                     \"items\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},                 },             },             \"required\": [\"index\", \"columns\", \"data\"],             \"type\": \"object\",         }      def parse_args(self, chunks: Iterable[str]) -&gt; pd.DataFrame:         \"\"\"Parse an iterable of string chunks into the function arguments.\"\"\"         args = json.loads(\"\".join(chunks))         return pd.DataFrame(**args)      def serialize_args(self, value: pd.DataFrame) -&gt; dict:         \"\"\"Serialize an object into a JSON string of function arguments.\"\"\"         return {             \"columns\": value.columns.tolist(),             \"data\": value.to_numpy().tolist(),         } In\u00a0[2]: Copied! <pre># Roundtrip test the new FunctionSchema\n\nfunction_schema = DataFrameFunctionSchema(pd.DataFrame)\n\ndf_test = pd.DataFrame(\n    {\n        \"A\": [1, 2, 3],\n        \"B\": [4, 5, 6],\n    },\n)\n\nargs = function_schema.serialize_args(df_test)\nprint(args)\n\nobj = function_schema.parse_args(json.dumps(args))\nobj\n</pre> # Roundtrip test the new FunctionSchema  function_schema = DataFrameFunctionSchema(pd.DataFrame)  df_test = pd.DataFrame(     {         \"A\": [1, 2, 3],         \"B\": [4, 5, 6],     }, )  args = function_schema.serialize_args(df_test) print(args)  obj = function_schema.parse_args(json.dumps(args)) obj <pre>{'columns': ['A', 'B'], 'data': [[1, 4], [2, 5], [3, 6]]}\n</pre> Out[2]: A B 0 1 4 1 2 5 2 3 6 In\u00a0[3]: Copied! <pre># Use pd.DataFrame as the return type of a prompt function\n\nimport pandas as pd\n\nfrom magentic import prompt\n\n\n@prompt(\n    \"Create a table listing the ingredients needed to cook {dish}.\"\n    \"Include a column for the quantity of each ingredient.\"\n    \"Also include a column with alergy information.\"\n)\ndef list_ingredients(dish: str) -&gt; pd.DataFrame: ...\n\n\nlist_ingredients(\"lasagna\")\n</pre> # Use pd.DataFrame as the return type of a prompt function  import pandas as pd  from magentic import prompt   @prompt(     \"Create a table listing the ingredients needed to cook {dish}.\"     \"Include a column for the quantity of each ingredient.\"     \"Also include a column with alergy information.\" ) def list_ingredients(dish: str) -&gt; pd.DataFrame: ...   list_ingredients(\"lasagna\") Out[3]: Ingredient Quantity Allergy Information 0 Lasagna noodles 16 oz Contains wheat, may contain egg and soy 1 Ground beef 1 lb Contains beef, may contain soy and gluten 2 Tomato sauce 24 oz Contains tomatoes, may contain soy and garlic 3 Mozzarella cheese 16 oz Contains milk, may contain soy 4 Ricotta cheese 15 oz Contains milk, may contain soy and eggs 5 Parmesan cheese 1 cup Contains milk, may contain soy and eggs 6 Garlic 3 cloves No known allergies 7 Onion 1 No known allergies 8 Olive oil 2 tbsp No known allergies 9 Salt 1 tsp No known allergies 10 Pepper 1/2 tsp No known allergies 11 Italian seasoning 1 tsp No known allergies 12 Sugar 1 tsp No known allergies"},{"location":"examples/registering_custom_type/#registering-a-custom-type","title":"Registering a Custom Type\u00b6","text":"<p>This notebook shows how to register a new type so that it can be used as the return annotation for <code>@prompt</code>, <code>@promptchain</code>, and <code>@chatprompt</code>. This is done by creating a new <code>FunctionSchema</code> which defines the parameters required to create the type, and how to parse/serialize these from/to the LLM.</p> <p>See https://platform.openai.com/docs/guides/function-calling for more information on function calling, which enables this.</p>"},{"location":"examples/vision_renaming_screenshots/","title":"Renaming Screenshots with GPT Vision","text":"In\u00a0[1]: Copied! <pre># List all screenshots\n\nfrom pathlib import Path\n\npath_desktop = Path.home() / \"Desktop\"\nscreenshot_paths = list(path_desktop.glob(\"Screenshot*.png\"))\n\nfor screenshot_path in screenshot_paths:\n    print(screenshot_path.name)\n</pre> # List all screenshots  from pathlib import Path  path_desktop = Path.home() / \"Desktop\" screenshot_paths = list(path_desktop.glob(\"Screenshot*.png\"))  for screenshot_path in screenshot_paths:     print(screenshot_path.name) <pre>Screenshot 2024-04-20 at 10.49.08\u202fPM Small.png\nScreenshot 2024-04-20 at 10.50.04\u202fPM Small.png\nScreenshot 2024-04-20 at 10.50.57\u202fPM Small.png\n</pre> In\u00a0[2]: Copied! <pre># Display the first screenshot\n\nfrom IPython.display import Image, display\n\n\ndef diplay_image(image_path):\n    display(Image(data=image_path.read_bytes(), width=400))\n\n\ndiplay_image(screenshot_paths[0])\n</pre> # Display the first screenshot  from IPython.display import Image, display   def diplay_image(image_path):     display(Image(data=image_path.read_bytes(), width=400))   diplay_image(screenshot_paths[0]) In\u00a0[3]: Copied! <pre># Define desired output for each screenshot\n# Include a description field to allow the LLM to think before naming the file\n\nfrom pydantic import BaseModel, Field\n\n\nclass ScreenshotDetails(BaseModel):\n    description: str = Field(\n        description=\"A brief description of the screenshot, including details that will be useful for naming it.\"\n    )\n    filename: str = Field(\n        description=\"An appropriate file name for this image, excluding the file extension.\"\n    )\n</pre> # Define desired output for each screenshot # Include a description field to allow the LLM to think before naming the file  from pydantic import BaseModel, Field   class ScreenshotDetails(BaseModel):     description: str = Field(         description=\"A brief description of the screenshot, including details that will be useful for naming it.\"     )     filename: str = Field(         description=\"An appropriate file name for this image, excluding the file extension.\"     ) In\u00a0[4]: Copied! <pre># Create a prompt-function to return details given an image\n\nfrom magentic import ImageBytes, OpenaiChatModel, Placeholder, UserMessage, chatprompt\n\n\n@chatprompt(\n    UserMessage(\n        [\n            \"Describe the screenshot, then provide a suitable file name.\",\n            Placeholder(ImageBytes, \"image\"),\n        ]\n    ),\n    model=OpenaiChatModel(\"gpt-4-turbo\"),\n)\ndef describe_image(image: bytes) -&gt; ScreenshotDetails: ...\n</pre> # Create a prompt-function to return details given an image  from magentic import ImageBytes, OpenaiChatModel, Placeholder, UserMessage, chatprompt   @chatprompt(     UserMessage(         [             \"Describe the screenshot, then provide a suitable file name.\",             Placeholder(ImageBytes, \"image\"),         ]     ),     model=OpenaiChatModel(\"gpt-4-turbo\"), ) def describe_image(image: bytes) -&gt; ScreenshotDetails: ... In\u00a0[5]: Copied! <pre># Rename all screenshots using the prompt-function\n\nfor path_screenshot in path_desktop.glob(\"Screenshot*.png\"):\n    print(path_screenshot.name)\n    diplay_image(path_screenshot)\n\n    image_bytes = path_screenshot.read_bytes()\n    image_details = describe_image(image_bytes)\n    print(image_details.description)\n\n    new_path = path_screenshot.with_stem(image_details.filename)\n    path_screenshot.rename(new_path)\n    print(\"\\nRenamed to:\", new_path.name)\n    print(\"\\n\\n---\\n\\n\")\n</pre> # Rename all screenshots using the prompt-function  for path_screenshot in path_desktop.glob(\"Screenshot*.png\"):     print(path_screenshot.name)     diplay_image(path_screenshot)      image_bytes = path_screenshot.read_bytes()     image_details = describe_image(image_bytes)     print(image_details.description)      new_path = path_screenshot.with_stem(image_details.filename)     path_screenshot.rename(new_path)     print(\"\\nRenamed to:\", new_path.name)     print(\"\\n\\n---\\n\\n\") <pre>Screenshot 2024-04-20 at 10.49.08\u202fPM Small.png\n</pre> <pre>The image shows the face of a white alpaca looking directly at the camera. The alpaca has a unique and stylish mohawk-like hairstyle. The background is blurred with a hint of green, suggesting an outdoor setting, likely a field.\n\nRenamed to: stylish-alpaca-face.png\n\n\n---\n\n\nScreenshot 2024-04-20 at 10.50.04\u202fPM Small.png\n</pre> <pre>A close-up image of a vibrant green snake coiled around a tree branch. The snake features a beautiful pattern of yellow spots and has a focused gaze. The background is softly blurred, emphasizing the snake in the foreground.\n\nRenamed to: green_snake_coiled_on_branch.png\n\n\n---\n\n\nScreenshot 2024-04-20 at 10.50.57\u202fPM Small.png\n</pre> <pre>The image displays a close-up view of a serving of lasagna on a white plate. The lasagna appears richly layered with melted cheese on top and a golden-brown crust, suggesting it is freshly baked and possibly contains a meaty sauce between the pasta sheets.\n\nRenamed to: close_up_lasagna_on_plate.png\n\n\n---\n\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/vision_renaming_screenshots/#renaming-screenshots-with-gpt-vision","title":"Renaming Screenshots with GPT Vision\u00b6","text":"<p>This notebook demonstrates how to use the vision capabilites of <code>gpt-4-turbo</code> in combination with magentic's structured outputs to rename all those screenshots cluttering your desktop.</p>"}]}